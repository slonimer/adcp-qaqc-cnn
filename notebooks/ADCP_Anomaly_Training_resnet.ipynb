{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Last Edited: 2025/10/29\n",
    "#File changed more recently (2025/11/18) to commit (with Cell outputs deleted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I created this ipynb file, and many of the .py files using the help of chatGPT.  \n",
    "#This was my initial prompt:\n",
    "\n",
    "'''\n",
    "So, I'd like to train an ML model.  I have a data set consisting of 4000 h5-format files.  Each file contains 24 hours of data.  There is a data group called '/data' with many multi channel variables.  There is also a set of annotations that indicate the classication of the data when anomalies are present.  Generally the class-imbalance for anomalous data is very large.  Probably <1 day with anomalous data per 300 days anomaly free. I want to do several things:\n",
    "\n",
    "1) Do pre-training on the \"good\"/\"anomaly free\" data\n",
    "2) Train on classified data.\n",
    "\n",
    "Things I will need to do:\n",
    "1) Have a data loader that loads the data\n",
    "- the data is for an ADCP with 3 beams.  Each beam collects data for beam velocity, beam backscatter, and beam correlation.  Since the beams are kind of independant, it probably makes sense to organize the data such that there are 3 input channels [velocity, backscatter, correlation], and each beam can be fed through the model independantly.  \n",
    "-The \"anomalous\" data may only occur for 6 hours within a 24 hour period, and I'd like the model to identify specifically which data is anomalous (not just binary yes/no for the entire 24 hour period).  I don't know if it makes sense to create a layer mask for the data, or if some kind of time-series label would be more appropriate (or even possible)\n",
    "\n",
    "2) Define a model architecture / type\n",
    " - I want this to be set up so that I can experiment with differnet options for this, and have this be pretty modular.  Any suggestions would be appreciated \n",
    " - I also want to have the option to be able to randomly tile the inputs, to add more variance for training, but this might not be needed\n",
    "\n",
    "3) Have a loss function that is weighted to account for class-imbalance.  I've used graduated dice loss in the past but not sure what other options are available\n",
    "\n",
    "'''\n",
    "\n",
    "#It then gave me a bunch of info, but no code, and I responded with \n",
    "\n",
    "'''\n",
    "so each input channel is 2D (time, range), and with multiple channels is 3D.  I guess I don't need to do anything fancy like making a mask for the annotations since it's not semantic segmentation, so the output could probably by 1D (time), with values like 0 (normal), 1, 2, 3, to indicate the class. Right now the annotations are start/end time and index, so I'll need to create a time-series label with the data loader.  \n",
    "\n",
    " I prefer pytorch.  I think a CNN is the minimum I'll want to use.  I'll also need a normalization step.  I like F1-score for an evaluation metric, and maybe a combo of cross entropy and dice loss, \n",
    "\n",
    "Can you give me code for all of this please?  Maybe start with the data loader. If there is a limit on tokens, do just the data loader, then ask me to say continue for each subsequent section of code\n",
    "'''\n",
    "\n",
    "#And from there it pretty much gave me all of this.  Now I have he task of testing and debugging the code and actually making it work for my data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "'''\n",
    "To set up the environment:\n",
    "\n",
    "conda create -n adcp_anomaly_env python=3.10\n",
    "conda activate adcp_anomaly_env\n",
    "pip install -r requirements.txt\n",
    "\n",
    "OR\n",
    "\n",
    "python -m venv adcp_anomaly_env\n",
    "adcp_anomaly_env\\Scripts\\activate\n",
    "pip install -r requirements.txt\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Imports & Setup\n",
    "\n",
    "import os\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "# Add repo root to Python path - Needed to import from src folder\n",
    "import sys\n",
    "from pathlib import Path\n",
    "repo_root = Path().resolve().parent  # notebooks/ → ADCP-CNN-QAQC\n",
    "sys.path.append(str(repo_root))\n",
    "\n",
    "from src.dataset_loader import ADCPDataset  # your custom dataset\n",
    "from src.resnet_temporal import ResNetTemporalClassifier # CNNClassifier  # your model\n",
    "# from model import TemporalCNN # CNNClassifier  # your model\n",
    "from src.utils import seed_everything, get_class_weights, combined_loss, train_model\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "seed_everything(42)\n",
    "\n",
    "USE_WANDB = False  # Set to True to enable logging - Keep false while in development/debugging\n",
    "if USE_WANDB:\n",
    "    import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Initialize wandb\n",
    "from types import SimpleNamespace\n",
    "\n",
    "if USE_WANDB:\n",
    "    wandb.init(project=\"adcp-anomaly-detection\", dir=r\"F:\\Documents\\GitHub\\ml_development\\ADCP_ML\\wandb_runs\", config={\n",
    "        \"model\": \"ResNetTemporalClassifier\",\n",
    "        \"epochs\": 20,\n",
    "        \"batch_size\": 16,\n",
    "        \"lr\": 1e-3,\n",
    "        \"loss_alpha\": 0.5,\n",
    "        \"optimizer\": \"Adam\"\n",
    "    })\n",
    "    config = wandb.config\n",
    "else:\n",
    "    config = SimpleNamespace(**{\n",
    "        \"model\": \"ResNetTemporalClassifier\",\n",
    "        \"epochs\": 20,\n",
    "        \"batch_size\": 2, #16,\n",
    "        \"lr\": 1e-3,\n",
    "        \"loss_alpha\": 0.5,\n",
    "        \"optimizer\": \"Adam\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F:\\Documents\\Projects\\ML\\ADCP_ML\\h5_24h_files\\\\20240406T000000_20240406T235959.h5\n"
     ]
    }
   ],
   "source": [
    "#DEBUG\n",
    "\n",
    "#Get a list of files from the directory:\n",
    "data_folder= r\"F:\\Documents\\Projects\\ML\\ADCP_ML\\h5_24h_files\\\\\"\n",
    "#annotation_file = \"/path/to/annotations.json\"\n",
    "file_list = os.listdir(data_folder)\n",
    "h5_files = {k for k in file_list if os.path.splitext(k)[1] == \".h5\"}\n",
    "h5_files = sorted(h5_files)  # Sorts alphabetically\n",
    "\n",
    "#print(os.path.splitext(file_list[0]))\n",
    "#print(h5_files)\n",
    "\n",
    "h5_paths = []\n",
    "for filename in h5_files:\n",
    "    h5_paths.append(data_folder + filename) \n",
    "\n",
    "#print(len(h5_paths))\n",
    "\n",
    "#WHILE DEBUGGING, ONLY US A FEW FILES\n",
    "#CHANGE THIS LATER\n",
    "file_idx = 3567\n",
    "print(h5_paths[file_idx])                                  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Load Dataset\n",
    "\n",
    "\n",
    "#Get a list of files from the directory:\n",
    "data_folder= r\"F:\\Documents\\Projects\\ML\\ADCP_ML\\h5_24h_files\\\\\"\n",
    "#annotation_file = \"/path/to/annotations.json\"\n",
    "file_list = os.listdir(data_folder)\n",
    "h5_files = {k for k in file_list if os.path.splitext(k)[1] == \".h5\"}\n",
    "#Files are inherently NOT in order in python! So if you want them in order, need to do this:\n",
    "h5_files = sorted(h5_files)  # Sorts alphabetically\n",
    "\n",
    "#print(os.path.splitext(file_list[0]))\n",
    "#print(h5_files)\n",
    "\n",
    "h5_paths = []\n",
    "for filename in h5_files:\n",
    "    h5_paths.append(data_folder + filename) \n",
    "\n",
    "#print(len(h5_paths))\n",
    "\n",
    "#WHILE DEBUGGING, ONLY US A FEW FILES\n",
    "#CHANGE THIS LATER\n",
    "\n",
    "file_idx = 3567\n",
    "h5_paths = h5_paths[file_idx-5 : file_idx+5 ] \n",
    "\n",
    "#num_files = 4\n",
    "#h5_paths = h5_paths[:num_files]                                             # <===== ELIMINATE THIS LATER    \n",
    "\n",
    "full_dataset = ADCPDataset(h5_paths) # (data_dir)\n",
    "#full_dataset = ADCPDataset(data_dir, annotation_file)\n",
    "train_size = int(0.8 * len(full_dataset))\n",
    "val_size = len(full_dataset) - train_size\n",
    "\n",
    "train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True, num_workers=4)\n",
    "val_loader = DataLoader(val_dataset, batch_size=config.batch_size, shuffle=False, num_workers=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "#Example:\n",
    "#if num_files is 20, then len(full_dataset) will be 60 (*3) because of the 3 beams\n",
    "\n",
    "print(train_size)\n",
    "print(val_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\slonimer\\AppData\\Local\\miniconda3\\envs\\adcp_anomaly_env\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\slonimer\\AppData\\Local\\miniconda3\\envs\\adcp_anomaly_env\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Initialize Model and Loss\n",
    "num_classes = 6 # full_dataset.num_classes\n",
    "model = ResNetTemporalClassifier(\n",
    "    num_classes=num_classes,\n",
    "    pretrained=True,      # set False if you want to train from scratch\n",
    "    variant='resnet50',     # options: 'resnet50', 'resnet101', 'resnet152'\n",
    "    resize=(224, 224)        # input size for ResNet\n",
    ")\n",
    "# model = TemporalCNN(input_channels=3, num_classes=num_classes)\n",
    "\n",
    "class_weights = torch.tensor([1.6761e-01, 3.8812e+01, 1.4015e+02, 9.8140e+02, 0.0000e+00, 0.0000e+00])\n",
    "#class_weights = torch.tensor([1/300, 1, 1, 1, 1, 1])                 # FIX THIS LATER -  TEMPORARY SETTING - DEFINE MANUALLY\n",
    "#class_weights = get_class_weights(train_dataset)                    # FIX THIS LATER -  UNCOMMENT THIS OR DEFINE MANUALLY BUT CORRECT WEIGHTS\n",
    "\n",
    "#Original with 6 classes\n",
    "#tensor([1.8522e-01, 2.0864e+00, 2.3713e+01, 9.4286e+01, 2.3760e+04, 1.4505e+01]) # For 200 files, is inverse of [5.3990, 0.4793, 0.0422, 0.0106, 0.0000,0.0689]\n",
    "# tensor([2.2450e-01, 3.8812e+01, 1.4015e+02, 9.8140e+02, 1.9692e+00, 9.9600e-01]) # For full dateset (or 70% anyways)\n",
    "\n",
    "loss_fn = combined_loss(class_weights, alpha=config.loss_alpha)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=config.lr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DEBUG: reload train_model\n",
    "\n",
    "import importlib\n",
    "import utils\n",
    "importlib.reload(utils)\n",
    "\n",
    "from utils import seed_everything, get_class_weights, combined_loss, train_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 288, 102])\n",
      "<class 'torch.Tensor'>\n",
      "torch.float32\n",
      "tensor(False)\n",
      "torch.Size([2, 3, 288, 102])\n",
      "output shape: torch.Size([2, 288, 6])\n",
      "y shape: torch.Size([2, 288])\n",
      "{'time': tensor([[1713225750, 1713226050, 1713226350, 1713226650, 1713226950, 1713227250,\n",
      "         1713227550, 1713227850, 1713228150, 1713228450, 1713228750, 1713229050,\n",
      "         1713229350, 1713229650, 1713229950, 1713230250, 1713230550, 1713230850,\n",
      "         1713231150, 1713231450, 1713231750, 1713232050, 1713232350, 1713232650,\n",
      "         1713232950, 1713233250, 1713233550, 1713233850, 1713234150, 1713234450,\n",
      "         1713234750, 1713235050, 1713235350, 1713235650, 1713235950, 1713236250,\n",
      "         1713236550, 1713236850, 1713237150, 1713237450, 1713237750, 1713238050,\n",
      "         1713238350, 1713238650, 1713238950, 1713239250, 1713239550, 1713239850,\n",
      "         1713240150, 1713240450, 1713240750, 1713241050, 1713241350, 1713241650,\n",
      "         1713241950, 1713242250, 1713242550, 1713242850, 1713243150, 1713243450,\n",
      "         1713243750, 1713244050, 1713244350, 1713244650, 1713244950, 1713245250,\n",
      "         1713245550, 1713245850, 1713246150, 1713246450, 1713246750, 1713247050,\n",
      "         1713247350, 1713247650, 1713247950, 1713248250, 1713248550, 1713248850,\n",
      "         1713249150, 1713249450, 1713249750, 1713250050, 1713250350, 1713250650,\n",
      "         1713250950, 1713251250, 1713251550, 1713251850, 1713252150, 1713252450,\n",
      "         1713252750, 1713253050, 1713253350, 1713253650, 1713253950, 1713254250,\n",
      "         1713254550, 1713254850, 1713255150, 1713255450, 1713255750, 1713256050,\n",
      "         1713256350, 1713256650, 1713256950, 1713257250, 1713257550, 1713257850,\n",
      "         1713258150, 1713258450, 1713258750, 1713259050, 1713259350, 1713259650,\n",
      "         1713259950, 1713260250, 1713260550, 1713260850, 1713261150, 1713261450,\n",
      "         1713261750, 1713262050, 1713262350, 1713262650, 1713262950, 1713263250,\n",
      "         1713263550, 1713263850, 1713264150, 1713264450, 1713264750, 1713265050,\n",
      "         1713265350, 1713265650, 1713265950, 1713266250, 1713266550, 1713266850,\n",
      "         1713267150, 1713267450, 1713267750, 1713268050, 1713268350, 1713268650,\n",
      "         1713268950, 1713269250, 1713269550, 1713269850, 1713270150, 1713270450,\n",
      "         1713270750, 1713271050, 1713271350, 1713271650, 1713271950, 1713272250,\n",
      "         1713272550, 1713272850, 1713273150, 1713273450, 1713273750, 1713274050,\n",
      "         1713274350, 1713274650, 1713274950, 1713275250, 1713275550, 1713275850,\n",
      "         1713276150, 1713276450, 1713276750, 1713277050, 1713277350, 1713277650,\n",
      "         1713277950, 1713278250, 1713278550, 1713278850, 1713279150, 1713279450,\n",
      "         1713279750, 1713280050, 1713280350, 1713280650, 1713280950, 1713281250,\n",
      "         1713281550, 1713281850, 1713282150, 1713282450, 1713282750, 1713283050,\n",
      "         1713283350, 1713283650, 1713283950, 1713284250, 1713284550, 1713284850,\n",
      "         1713285150, 1713285450, 1713285750, 1713286050, 1713286350, 1713286650,\n",
      "         1713286950, 1713287250, 1713287550, 1713287850, 1713288150, 1713288450,\n",
      "         1713288750, 1713289050, 1713289350, 1713289650, 1713289950, 1713290250,\n",
      "         1713290550, 1713290850, 1713291150, 1713291450, 1713291750, 1713292050,\n",
      "         1713292350, 1713292650, 1713292950, 1713293250, 1713293550, 1713293850,\n",
      "         1713294150, 1713294450, 1713294750, 1713295050, 1713295350, 1713295650,\n",
      "         1713295950, 1713296250, 1713296550, 1713296850, 1713297150, 1713297450,\n",
      "         1713297750, 1713298050, 1713298350, 1713298650, 1713298950, 1713299250,\n",
      "         1713299550, 1713299850, 1713300150, 1713300450, 1713300750, 1713301050,\n",
      "         1713301350, 1713301650, 1713301950, 1713302250, 1713302550, 1713302850,\n",
      "         1713303150, 1713303450, 1713303750, 1713304050, 1713304350, 1713304650,\n",
      "         1713304950, 1713305250, 1713305550, 1713305850, 1713306150, 1713306450,\n",
      "         1713306750, 1713307050, 1713307350, 1713307650, 1713307950, 1713308250,\n",
      "         1713308550, 1713308850, 1713309150, 1713309450, 1713309750, 1713310050,\n",
      "         1713310350, 1713310650, 1713310950, 1713311250, 1713311550, 1713311850],\n",
      "        [1712707350, 1712707650, 1712707950, 1712708250, 1712708550, 1712708850,\n",
      "         1712709150, 1712709450, 1712709750, 1712710050, 1712710350, 1712710650,\n",
      "         1712710950, 1712711250, 1712711550, 1712711850, 1712712150, 1712712450,\n",
      "         1712712750, 1712713050, 1712713350, 1712713650, 1712713950, 1712714250,\n",
      "         1712714550, 1712714850, 1712715150, 1712715450, 1712715750, 1712716050,\n",
      "         1712716350, 1712716650, 1712716950, 1712717250, 1712717550, 1712717850,\n",
      "         1712718150, 1712718450, 1712718750, 1712719050, 1712719350, 1712719650,\n",
      "         1712719950, 1712720250, 1712720550, 1712720850, 1712721150, 1712721450,\n",
      "         1712721750, 1712722050, 1712722350, 1712722650, 1712722950, 1712723250,\n",
      "         1712723550, 1712723850, 1712724150, 1712724450, 1712724750, 1712725050,\n",
      "         1712725350, 1712725650, 1712725950, 1712726250, 1712726550, 1712726850,\n",
      "         1712727150, 1712727450, 1712727750, 1712728050, 1712728350, 1712728650,\n",
      "         1712728950, 1712729250, 1712729550, 1712729850, 1712730150, 1712730450,\n",
      "         1712730750, 1712731050, 1712731350, 1712731650, 1712731950, 1712732250,\n",
      "         1712732550, 1712732850, 1712733150, 1712733450, 1712733750, 1712734050,\n",
      "         1712734350, 1712734650, 1712734950, 1712735250, 1712735550, 1712735850,\n",
      "         1712736150, 1712736450, 1712736750, 1712737050, 1712737350, 1712737650,\n",
      "         1712737950, 1712738250, 1712738550, 1712738850, 1712739150, 1712739450,\n",
      "         1712739750, 1712740050, 1712740350, 1712740650, 1712740950, 1712741250,\n",
      "         1712741550, 1712741850, 1712742150, 1712742450, 1712742750, 1712743050,\n",
      "         1712743350, 1712743650, 1712743950, 1712744250, 1712744550, 1712744850,\n",
      "         1712745150, 1712745450, 1712745750, 1712746050, 1712746350, 1712746650,\n",
      "         1712746950, 1712747250, 1712747550, 1712747850, 1712748150, 1712748450,\n",
      "         1712748750, 1712749050, 1712749350, 1712749650, 1712749950, 1712750250,\n",
      "         1712750550, 1712750850, 1712751150, 1712751450, 1712751750, 1712752050,\n",
      "         1712752350, 1712752650, 1712752950, 1712753250, 1712753550, 1712753850,\n",
      "         1712754150, 1712754450, 1712754750, 1712755050, 1712755350, 1712755650,\n",
      "         1712755950, 1712756250, 1712756550, 1712756850, 1712757150, 1712757450,\n",
      "         1712757750, 1712758050, 1712758350, 1712758650, 1712758950, 1712759250,\n",
      "         1712759550, 1712759850, 1712760150, 1712760450, 1712760750, 1712761050,\n",
      "         1712761350, 1712761650, 1712761950, 1712762250, 1712762550, 1712762850,\n",
      "         1712763150, 1712763450, 1712763750, 1712764050, 1712764350, 1712764650,\n",
      "         1712764950, 1712765250, 1712765550, 1712765850, 1712766150, 1712766450,\n",
      "         1712766750, 1712767050, 1712767350, 1712767650, 1712767950, 1712768250,\n",
      "         1712768550, 1712768850, 1712769150, 1712769450, 1712769750, 1712770050,\n",
      "         1712770350, 1712770650, 1712770950, 1712771250, 1712771550, 1712771850,\n",
      "         1712772150, 1712772450, 1712772750, 1712773050, 1712773350, 1712773650,\n",
      "         1712773950, 1712774250, 1712774550, 1712774850, 1712775150, 1712775450,\n",
      "         1712775750, 1712776050, 1712776350, 1712776650, 1712776950, 1712777250,\n",
      "         1712777550, 1712777850, 1712778150, 1712778450, 1712778750, 1712779050,\n",
      "         1712779350, 1712779650, 1712779950, 1712780250, 1712780550, 1712780850,\n",
      "         1712781150, 1712781450, 1712781750, 1712782050, 1712782350, 1712782650,\n",
      "         1712782950, 1712783250, 1712783550, 1712783850, 1712784150, 1712784450,\n",
      "         1712784750, 1712785050, 1712785350, 1712785650, 1712785950, 1712786250,\n",
      "         1712786550, 1712786850, 1712787150, 1712787450, 1712787750, 1712788050,\n",
      "         1712788350, 1712788650, 1712788950, 1712789250, 1712789550, 1712789850,\n",
      "         1712790150, 1712790450, 1712790750, 1712791050, 1712791350, 1712791650,\n",
      "         1712791950, 1712792250, 1712792550, 1712792850, 1712793150, 1712793450]]), 'filename': ['F:\\\\Documents\\\\Projects\\\\ML\\\\ADCP_ML\\\\h5_24h_files\\\\\\\\20240416T000000_20240416T235959.h5', 'F:\\\\Documents\\\\Projects\\\\ML\\\\ADCP_ML\\\\h5_24h_files\\\\\\\\20240410T000000_20240410T235959.h5'], 'channels': [('velocity', 'velocity'), ('backscatter', 'backscatter'), ('correlation', 'correlation')]}\n"
     ]
    }
   ],
   "source": [
    "# DEBUG:  One batch from your DataLoader\n",
    "\n",
    "import os\n",
    "os.environ[\"TORCH_DISABLE_MKL\"] = \"1\"  # optional: disables MKL\n",
    "os.environ[\"ONEDNN_VERBOSE\"] = \"0\"\n",
    "os.environ[\"DNNL_VERBOSE\"] = \"0\"\n",
    "torch.backends.mkldnn.enabled = False\n",
    "\n",
    "\n",
    "x_batch, y_batch, meta_batch = next(iter(train_loader))\n",
    "\n",
    "print(x_batch.shape)  # should be (B, T, num_classes)\n",
    "\n",
    "print(type(x_batch))             # Should be torch.Tensor\n",
    "print(x_batch.dtype)             # Should be torch.float32\n",
    "print(torch.isnan(x_batch).any())  # Should be False\n",
    "print(x_batch.shape)             # Should be [B, 3, T, R]\n",
    "\n",
    "x_batch = x_batch.to(device)\n",
    "model = model.to(device)\n",
    "\n",
    "# Run forward pass\n",
    "with torch.no_grad():\n",
    "   outputs = model(x_batch) # (B, T, num_classes)\n",
    "\n",
    "#optimizer.zero_grad()\n",
    "#outputs = model(x_batch)     \n",
    "print(\"output shape:\", outputs.shape)\n",
    "print(\"y shape:\", y_batch.shape)\n",
    "\n",
    "#Need to reshape the outputs, and y_batch for loss_fn to work properly:\n",
    "outputs = outputs.reshape(-1, outputs.shape[-1])  # (B*T, num_classes)\n",
    "y_batch = y_batch.view(-1)                        # (B*T, )\n",
    "\n",
    "#Run the loss function\n",
    "loss_fn = combined_loss(class_weights, alpha=config.loss_alpha)\n",
    "loss = loss_fn(outputs, y_batch)\n",
    "\n",
    "#Check the meta data contents \n",
    "print(meta_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.4480)\n"
     ]
    }
   ],
   "source": [
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training on Epoch #  0\n",
      "Loss (& total) on Batch #1: 1.4090192317962646 (1.4090192317962646)\n",
      "Loss (& total) on Batch #2: 0.8244938254356384 (2.233513057231903)\n",
      "Loss (& total) on Batch #3: 0.5441804528236389 (2.777693510055542)\n",
      "Loss (& total) on Batch #4: 0.5083618760108948 (3.2860553860664368)\n",
      "Loss (& total) on Batch #5: 0.5015065670013428 (3.7875619530677795)\n",
      "Loss (& total) on Batch #6: 0.5005192756652832 (4.288081228733063)\n",
      "Loss (& total) on Batch #7: 0.5002608895301819 (4.788342118263245)\n",
      "Loss (& total) on Batch #8: 0.500089704990387 (5.288431823253632)\n",
      "Loss (& total) on Batch #9: 0.5000426769256592 (5.788474500179291)\n",
      "Loss (& total) on Batch #10: 0.5000271201133728 (6.288501620292664)\n",
      "Loss (& total) on Batch #11: 0.5000137686729431 (6.788515388965607)\n",
      "Loss (& total) on Batch #12: 0.5000052452087402 (7.288520634174347)\n",
      "Starting Validation on Epoch #  0\n",
      "Epoch 1/20 | Train Loss: 0.6074 | Val Loss: 0.5000 | Val F1: 1.0000\n",
      "✅ New best model saved.\n",
      "Starting Training on Epoch #  1\n",
      "Loss (& total) on Batch #1: 0.5000038743019104 (0.5000038743019104)\n",
      "Loss (& total) on Batch #2: 0.5000019073486328 (1.0000057816505432)\n",
      "Loss (& total) on Batch #3: 0.4999990463256836 (1.5000048279762268)\n",
      "Loss (& total) on Batch #4: 0.4999985694885254 (2.000003397464752)\n",
      "Loss (& total) on Batch #5: 0.4999980032444 (2.500001400709152)\n",
      "Loss (& total) on Batch #6: 0.4999973475933075 (2.9999987483024597)\n",
      "Loss (& total) on Batch #7: 0.49999701976776123 (3.499995768070221)\n",
      "Loss (& total) on Batch #8: 0.49999698996543884 (3.99999275803566)\n",
      "Loss (& total) on Batch #9: 0.49999669194221497 (4.499989449977875)\n",
      "Loss (& total) on Batch #10: 0.49999669194221497 (4.99998614192009)\n",
      "Loss (& total) on Batch #11: 0.499996542930603 (5.499982684850693)\n",
      "Loss (& total) on Batch #12: 0.4999964237213135 (5.999979108572006)\n",
      "Starting Validation on Epoch #  1\n",
      "Epoch 2/20 | Train Loss: 0.5000 | Val Loss: 0.5000 | Val F1: 1.0000\n",
      "Starting Training on Epoch #  2\n",
      "Loss (& total) on Batch #1: 0.4999964237213135 (0.4999964237213135)\n",
      "Loss (& total) on Batch #2: 0.49999645352363586 (0.9999928772449493)\n",
      "Loss (& total) on Batch #3: 0.4999964237213135 (1.4999893009662628)\n",
      "Loss (& total) on Batch #4: 0.4999963045120239 (1.9999856054782867)\n",
      "Loss (& total) on Batch #5: 0.4999963939189911 (2.499981999397278)\n",
      "Loss (& total) on Batch #6: 0.49999627470970154 (2.9999782741069794)\n",
      "Loss (& total) on Batch #7: 0.4999963045120239 (3.4999745786190033)\n",
      "Loss (& total) on Batch #8: 0.49999627470970154 (3.999970853328705)\n",
      "Loss (& total) on Batch #9: 0.4999963045120239 (4.499967157840729)\n",
      "Loss (& total) on Batch #10: 0.4999964237213135 (4.999963581562042)\n",
      "Loss (& total) on Batch #11: 0.49999624490737915 (5.499959826469421)\n",
      "Loss (& total) on Batch #12: 0.49999627470970154 (5.999956101179123)\n",
      "Starting Validation on Epoch #  2\n",
      "Epoch 3/20 | Train Loss: 0.5000 | Val Loss: 0.5000 | Val F1: 1.0000\n",
      "Starting Training on Epoch #  3\n",
      "Loss (& total) on Batch #1: 0.49999627470970154 (0.49999627470970154)\n",
      "Loss (& total) on Batch #2: 0.4999963641166687 (0.9999926388263702)\n",
      "Loss (& total) on Batch #3: 0.4999961853027344 (1.4999888241291046)\n",
      "Loss (& total) on Batch #4: 0.49999624490737915 (1.9999850690364838)\n",
      "Loss (& total) on Batch #5: 0.4999961853027344 (2.499981254339218)\n",
      "Loss (& total) on Batch #6: 0.49999621510505676 (2.999977469444275)\n",
      "Loss (& total) on Batch #7: 0.49999624490737915 (3.499973714351654)\n",
      "Loss (& total) on Batch #8: 0.49999624490737915 (3.999969959259033)\n",
      "Loss (& total) on Batch #9: 0.4999961853027344 (4.499966144561768)\n",
      "Loss (& total) on Batch #10: 0.4999961853027344 (4.999962329864502)\n",
      "Loss (& total) on Batch #11: 0.4999961853027344 (5.499958515167236)\n",
      "Loss (& total) on Batch #12: 0.4999961853027344 (5.999954700469971)\n",
      "Starting Validation on Epoch #  3\n",
      "Epoch 4/20 | Train Loss: 0.5000 | Val Loss: 0.5000 | Val F1: 1.0000\n",
      "Starting Training on Epoch #  4\n",
      "Loss (& total) on Batch #1: 0.4999961853027344 (0.4999961853027344)\n",
      "Loss (& total) on Batch #2: 0.49999621510505676 (0.9999924004077911)\n",
      "Loss (& total) on Batch #3: 0.49999627470970154 (1.4999886751174927)\n",
      "Loss (& total) on Batch #4: 0.4999961853027344 (1.999984860420227)\n",
      "Loss (& total) on Batch #5: 0.4999961853027344 (2.4999810457229614)\n",
      "Loss (& total) on Batch #6: 0.49999621510505676 (2.999977260828018)\n",
      "Loss (& total) on Batch #7: 0.4999961853027344 (3.4999734461307526)\n",
      "Loss (& total) on Batch #8: 0.4999961853027344 (3.999969631433487)\n",
      "Loss (& total) on Batch #9: 0.4999961853027344 (4.499965816736221)\n",
      "Loss (& total) on Batch #10: 0.49999621510505676 (4.999962031841278)\n",
      "Loss (& total) on Batch #11: 0.4999961853027344 (5.4999582171440125)\n",
      "Loss (& total) on Batch #12: 0.4999961853027344 (5.999954402446747)\n",
      "Starting Validation on Epoch #  4\n",
      "Epoch 5/20 | Train Loss: 0.5000 | Val Loss: 0.5000 | Val F1: 1.0000\n",
      "Starting Training on Epoch #  5\n",
      "Loss (& total) on Batch #1: 0.49999621510505676 (0.49999621510505676)\n",
      "Loss (& total) on Batch #2: 0.49999621510505676 (0.9999924302101135)\n",
      "Loss (& total) on Batch #3: 0.499996155500412 (1.4999885857105255)\n",
      "Loss (& total) on Batch #4: 0.4999963045120239 (1.9999848902225494)\n",
      "Loss (& total) on Batch #5: 0.4999961853027344 (2.499981075525284)\n",
      "Loss (& total) on Batch #6: 0.499996155500412 (2.999977231025696)\n",
      "Loss (& total) on Batch #7: 0.4999961853027344 (3.49997341632843)\n",
      "Loss (& total) on Batch #8: 0.499996155500412 (3.999969571828842)\n",
      "Loss (& total) on Batch #9: 0.49999621510505676 (4.499965786933899)\n",
      "Loss (& total) on Batch #10: 0.49999621510505676 (4.999962002038956)\n",
      "Loss (& total) on Batch #11: 0.4999961853027344 (5.49995818734169)\n",
      "Loss (& total) on Batch #12: 0.49999621510505676 (5.999954402446747)\n",
      "Starting Validation on Epoch #  5\n",
      "Epoch 6/20 | Train Loss: 0.5000 | Val Loss: 0.5000 | Val F1: 1.0000\n",
      "⏹️ Early stopping triggered.\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Train\n",
    "\n",
    "#I was having bugs in this, where it was saying it failed creating a primitive. This was found to solve the issue (but shouldn't be used when proper training/running)\n",
    "import os\n",
    "os.environ[\"TORCH_DISABLE_MKL\"] = \"1\"  # optional: disables MKL\n",
    "os.environ[\"ONEDNN_VERBOSE\"] = \"0\"\n",
    "os.environ[\"DNNL_VERBOSE\"] = \"0\"\n",
    "torch.backends.mkldnn.enabled = False\n",
    "\n",
    "if USE_WANDB:\n",
    "    model = train_model(model, train_loader, val_loader, optimizer, loss_fn, device, num_epochs=config.epochs, patience=5)\n",
    "else:\n",
    "    model, history = train_model(model, train_loader, val_loader, optimizer, loss_fn, device, num_epochs=config.epochs, patience=5)\n",
    "\n",
    "# ([2, 3, 288, 102])\n",
    "# => [batch, channels, time, range]\n",
    "    \n",
    "#Best result using ce and dice-loss:\n",
    "#Epoch 7/20 | Train Loss: 0.7003 | Val Loss: 0.9850 | Val F1: 0.4931\n",
    "#⏹️ Early stopping triggered.\n",
    "    \n",
    "# With ce and graduated dice-loss:\n",
    "# Starting Validation on Epoch #  6\n",
    "# Epoch 7/20 | Train Loss: 0.7778 | Val Loss: 0.9043 | Val F1: 0.4931\n",
    "# #\n",
    "# Starting Validation on Epoch #  7\n",
    "# Epoch 8/20 | Train Loss: 0.7598 | Val Loss: 0.9866 | Val F1: 0.4931\n",
    "# ⏹️ Early stopping triggered.\n",
    "\n",
    "#It's weird that the val F1 is identical for all of these. I wonder if something is going on, but hard to say"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1728\n",
      "\n",
      "    accuracy                           1.00      1728\n",
      "   macro avg       1.00      1.00      1.00      1728\n",
      "weighted avg       1.00      1.00      1.00      1728\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Load Best Model and Evaluate\n",
    "model.load_state_dict(torch.load(\"best_model.pt\"))\n",
    "model.eval()\n",
    "\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "for x, y, _ in val_loader:\n",
    "    x = x.to(device)\n",
    "    with torch.no_grad():\n",
    "        out = model(x)\n",
    "        #Need to reshape the outputs, and y to match dimensions:\n",
    "        out = out.reshape(-1, out.shape[-1])  # (B*T, num_classes)\n",
    "        #The prediction is the class with largest score per sample\n",
    "        preds = torch.argmax(out, dim=1)\n",
    "\n",
    "    #Need to reshape the outputs, and y to match dimensions:\n",
    "    y = y.view(-1)                        # (B*T, )\n",
    "\n",
    "    #Append the results\n",
    "    all_preds.append(preds.cpu())\n",
    "    all_labels.append(y)\n",
    "\n",
    "y_pred = torch.cat(all_preds).numpy()\n",
    "y_true = torch.cat(all_labels).numpy()\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_true, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1728,)\n",
      "(1728,)\n"
     ]
    }
   ],
   "source": [
    "print(y_pred.shape)\n",
    "print(y_true.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([   0,    1,    2, ..., 1725, 1726, 1727], shape=(1728,)),)\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print(np.where(y_true==0))\n",
    "print(np.all(y_true==0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F:\\Documents\\Projects\\ML\\ADCP_ML\\h5_24h_files\\\\20240406T000000_20240406T235959.h5\n"
     ]
    }
   ],
   "source": [
    "print(h5_test_file) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\slonimer\\AppData\\Local\\miniconda3\\envs\\adcp_anomaly_env\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\slonimer\\AppData\\Local\\miniconda3\\envs\\adcp_anomaly_env\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "#Cell 7: Make test plots.  \n",
    "#I want to push a file through the algorithm, and see how it performs\n",
    "\n",
    "#IMPORT EVERYTHING:\n",
    "import os\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "from src.dataset_loader import ADCPDataset  # your custom dataset\n",
    "from src.model import TemporalCNN # CNNClassifier  # your model\n",
    "from src.utils import seed_everything, get_class_weights, combined_loss, train_model\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "#DEBUG:\n",
    "#I was having bugs in this, where it was saying it failed creating a primitive. This was found to solve the issue (but shouldn't be used when proper training/running)\n",
    "import os\n",
    "os.environ[\"TORCH_DISABLE_MKL\"] = \"1\"  # optional: disables MKL\n",
    "os.environ[\"ONEDNN_VERBOSE\"] = \"0\"\n",
    "os.environ[\"DNNL_VERBOSE\"] = \"0\"\n",
    "torch.backends.mkldnn.enabled = False\n",
    "\n",
    "\n",
    "\n",
    "# INITIALIZE THE MODEL\n",
    "model_path = r\"F:\\Documents\\GitHub\\ml_development\\ADCP_ML\\\\\"\n",
    "num_classes = 6 \n",
    "#from resnet_temporal import ResNetTemporalClassifier # CNNClassifier  # your model\n",
    "model = ResNetTemporalClassifier(\n",
    "    num_classes=num_classes,\n",
    "    pretrained=True,      # set False if you want to train from scratch\n",
    "    variant='resnet50',     # options: 'resnet50', 'resnet101', 'resnet152'\n",
    "    resize=(224, 224)        # input size for ResNet\n",
    ")\n",
    "#model = TemporalCNN(input_channels=3, num_classes=num_classes)\n",
    "model.load_state_dict(torch.load(model_path + \"best_model_20251028_resnet50.pt\", map_location=torch.device('cpu') ))\n",
    "#model.load_state_dict(torch.load(model_path + \"best_model_20250508.pt\", map_location=torch.device('cpu') ))\n",
    "#model.load_state_dict(torch.load(model_path + \"best_model_20250502.pt\", map_location=torch.device('cpu') ))\n",
    "model.eval()\n",
    "\n",
    "\n",
    "#LOAD THE TEST FILE\n",
    "file_path = r\"F:\\Documents\\Projects\\ML\\ADCP_ML\\h5_24h_files\\\\\"\n",
    "h5_filename = '20240406T000000_20240406T235959.h5'\n",
    "#\n",
    "h5_test_file = []\n",
    "h5_test_file.append(file_path + h5_filename) \n",
    "\n",
    "\n",
    "#Create a generic function for classifying files\n",
    "def classify_test_data(model, h5_test_file):\n",
    "    test_file_dataset = ADCPDataset(h5_test_file)\n",
    "    test_loader = DataLoader(test_file_dataset, batch_size=3, shuffle=False, num_workers=4)\n",
    "\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    for x, y, meta in test_loader:\n",
    "        x = x.to(device)\n",
    "        model = model.to(device)  # ← Add this line\n",
    "        with torch.no_grad():\n",
    "            out = model(x)\n",
    "            #Need to reshape the outputs, and y to match dimensions:\n",
    "            out = out.reshape(-1, out.shape[-1])  # (B*T, num_classes)\n",
    "            #The prediction is the class with largest score per sample\n",
    "            preds = torch.argmax(out, dim=1)\n",
    "\n",
    "        #Need to reshape the outputs, and y to match dimensions:\n",
    "        y = y.view(-1)                        # (B*T, )\n",
    "\n",
    "        #Append the results\n",
    "        all_preds.append(preds.cpu())\n",
    "        all_labels.append(y)\n",
    "        \n",
    "    return x, y, preds, meta\n",
    "\n",
    "\n",
    "#Run the classification\n",
    "x, y, preds, meta = classify_test_data(model, h5_test_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1712361750)\n",
      "tensor(1712361750)\n",
      "tensor(1712361750)\n",
      "velocity\n",
      "backscatter\n"
     ]
    }
   ],
   "source": [
    "#DEBUG\n",
    "#meta[\"time\"][0].size()\n",
    "print(meta[\"time\"][0,0])\n",
    "print(meta[\"time\"][1,0])\n",
    "print(meta[\"time\"][2,0])\n",
    "#So, I just need meta[\"time\"][0]\n",
    "\n",
    "print(meta[\"channels\"][0][0])\n",
    "print(meta[\"channels\"][1][0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      1.00      0.97       817\n",
      "           1       0.00      0.00      0.00        47\n",
      "\n",
      "    accuracy                           0.95       864\n",
      "   macro avg       0.47      0.50      0.49       864\n",
      "weighted avg       0.89      0.95      0.92       864\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\slonimer\\AppData\\Local\\miniconda3\\envs\\adcp_anomaly_env\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\slonimer\\AppData\\Local\\miniconda3\\envs\\adcp_anomaly_env\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\slonimer\\AppData\\Local\\miniconda3\\envs\\adcp_anomaly_env\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 3, 288, 102])\n",
      "torch.Size([864])\n"
     ]
    }
   ],
   "source": [
    "print(x.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([864])\n"
     ]
    }
   ],
   "source": [
    "#print(test_file_dataset[1])\n",
    "#print(x[2])\n",
    "print(preds.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "#I think this is a bug! I think annotations are being applied to all beams instead of just one!\n",
    "# => RESOLVED\n",
    "annotations = y.view(3,288)\n",
    "for an in annotations:\n",
    "    print(an)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NEW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import numpy as np\n",
    "import datetime\n",
    "\n",
    "\n",
    "def get_segments(annotations, ann):\n",
    "    if ann==1: #For annotations (may be multiclass)\n",
    "        mask = np.diff(annotations) != 0 # Create a mask of where changes in annotations are non-zero\n",
    "        #diffs = mask.astype(int)\n",
    "    elif ann==0: #For predictions\n",
    "        mask = annotations != 0 # Create a mask of where annotations are non-zero\n",
    "        \n",
    "    diffs = np.diff(mask.astype(int)) # Find the changes in the mask\n",
    "    start_indices = np.where(diffs == 1)[0] + 1 # Start indices: where diff == 1 (0 → 1)\n",
    "    end_indices = np.where(diffs == -1)[0] + 1 # End indices: where diff == -1 (1 → 0)\n",
    "    # Handle edge cases: \n",
    "    if mask[0]: #if it starts with a non-zero \n",
    "        start_indices = np.r_[0, start_indices]\n",
    "\n",
    "    if mask[-1]: # if it ends with a non-zero\n",
    "        end_indices = np.r_[end_indices, len(annotations)]\n",
    "\n",
    "    anomaly_segments = list(zip(start_indices, end_indices)) # Zip together\n",
    "    return anomaly_segments\n",
    "\n",
    "\n",
    "def plot_results(x, annotations, predictions, filename, meta) :\n",
    "    x = x.cpu()\n",
    "    annotations = annotations.cpu()\n",
    "    predictions = predictions.cpu()\n",
    "\n",
    "    n_beams = x.shape[0]#[2]\n",
    "    n_channels = x.shape[1]#[2]\n",
    "\n",
    "    #time_data = meta['time'][0]\n",
    "    time_data = [datetime.datetime.utcfromtimestamp(t.item()) for t in meta['time'][0]]\n",
    "\n",
    "    for beam in range(n_beams):\n",
    "        fig, axs = plt.subplots(n_channels, 1, sharex=True, figsize=(12, 2.5*n_channels))\n",
    "        if n_channels == 1:\n",
    "            axs = [axs]\n",
    "\n",
    "        #Get the annotations for this beam\n",
    "        anomaly_segments = get_segments(annotations[beam].cpu().numpy(),ann = 1)\n",
    "        pred_segments = get_segments(predictions[beam].cpu().numpy(), ann = 0)\n",
    "\n",
    "        #print(anomaly_segments)\n",
    "        #print(pred_segments)\n",
    "\n",
    "        #Determine if any annotations present in this beam:\n",
    "        cls_str = '' #Initialize as nothing\n",
    "        ann = annotations[beam].cpu().numpy()\n",
    "        if np.any(ann>0):\n",
    "            cls = np.median(ann[ann>0])\n",
    "            cls_str = ', class: {}'.format(int(cls))\n",
    "\n",
    "\n",
    "        #Plot Velocity, backscatter, and correlation, for each beam\n",
    "        for ch in range(n_channels):\n",
    "            #Plot the Complex Data\n",
    "            im = axs[ch].imshow(\n",
    "                x[beam,ch,:,:].T, aspect='auto', origin='lower',\n",
    "                #extent=[extent[0], extent[1], extent[2], extent[3]],\n",
    "                extent=[time_data[0], time_data[-1], 0, x.shape[3]-1],\n",
    "                interpolation='nearest',\n",
    "                cmap='jet',\n",
    "            )\n",
    "\n",
    "            #Set the figure title\n",
    "            if beam == 0 and ch == 0:\n",
    "                fig.suptitle('Annotions = Shaded, Predictions = --')\n",
    "                #fig.suptitle('File: {}'.format(filename))\n",
    "            \n",
    "            #Set the subplot titles\n",
    "            if ch == 0:\n",
    "                axs[ch].set_title('Beam #{} {}'.format(beam+1, cls_str))   \n",
    "           \n",
    "            #Add labels and titles\n",
    "            #axs[ch].set_ylabel(\"Range bin\" if range_dim is not None else '')\n",
    "            #axs[ch].set_title(f\"{var} - Channel {ch+1}\")\n",
    "\n",
    "            #Add dashed vertical lines for predictions\n",
    "            for start, end in pred_segments:\n",
    "                if 0 <= start < x.shape[2]:\n",
    "                    axs[ch].axvline(x=time_data[start], color='black', linestyle='dashed', alpha=0.7)\n",
    "                    #axs[ch].axvline(x=start, color='red', linestyle='dashed', alpha=0.7)\n",
    "                if 0 <= end < x.shape[2]:\n",
    "                    axs[ch].axvline(x=time_data[end], color='black', linestyle='dashed', alpha=0.7)\n",
    "                    #axs[ch].axvline(x=end, color='red', linestyle='dashed', alpha=0.7)\n",
    "\n",
    "            #Add shading for annotations\n",
    "            for start, end in anomaly_segments:\n",
    "                if 0 <= start < x.shape[2] and 0 <= end <= x.shape[2]:\n",
    "                    axs[ch].axvspan(time_data[start], time_data[end], color='black', alpha=0.3)\n",
    "                    #axs[ch].axvspan(start, end, color='black', alpha=0.3)\n",
    "\n",
    "            #Add a colorbar\n",
    "            fig.colorbar(im, ax=axs[ch], label=meta['channels'][ch][0])\n",
    "            #fig.colorbar(im, ax=axs[ch], label='color')\n",
    "\n",
    "            #Add y-axis label\n",
    "            axs[ch].set_ylabel('{}'.format('Range [m]') )\n",
    "\n",
    "\n",
    "        # -- Date formatting for X --\n",
    "        axs[-1].xaxis_date()  # tells matplotlib to interpret x as dates\n",
    "        axs[-1].xaxis.set_major_formatter(mdates.DateFormatter('%H:%M'))\n",
    "        fig.autofmt_xdate()  # Makes dates pretty (auto-rotates, etc.)\n",
    "\n",
    "        #Add x-axis label\n",
    "        axs[-1].set_xlabel('{} [HH:MM] UTC'.format(time_data[0].strftime('%Y-%m-%d')) )\n",
    "\n",
    "        #axs[-1].set_xlabel(time_dt[0].astype('datetime64[D]').astype(str))   # 'yyyy-mm-dd' date for xlabel\n",
    "        #axs[-1].set_xlabel(\"Time (hours since start)\")\n",
    "        #fig.suptitle(f\"{var} (shape={arr.shape})\")\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        #if outdir:\n",
    "        #    if not os.path.exists(outdir):\n",
    "        #        os.makedirs(outdir)\n",
    "        #    plt.savefig(f\"{outdir}/{var}.png\", dpi=120)\n",
    "        #if show:\n",
    "        #    plt.show()\n",
    "        plt.show()\n",
    "        #plt.close()\n",
    "\n",
    "        \n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get indices of start/end segments of anomalies\n",
    "annotations = y.view(3,288)\n",
    "predictions = preds.view(3,288)\n",
    "\n",
    "plot_results(x, annotations, predictions, h5_filename, meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Can I apply this to data from a DIFFERENT SITE? From another device??\n",
    "\n",
    "#This was trained/tested on data from BACAX, \n",
    "\n",
    "#I want to make an example figure for Drew, for BACUS, 2024-08-08\n",
    "\n",
    "data_parent = r'F:\\Documents\\Projects\\ADCP\\scan_for_data\\BACUS\\ADCP2MHZ\\\\'\n",
    "folder_list = ['20240801']\n",
    "#folder_list = os.listdir(data_parent)\n",
    "\n",
    "#Define folders\n",
    "h5_monthly_folder = r'F:\\Documents\\Projects\\ML\\ADCP_ML\\BACUS\\h5_files\\\\' # Define output folder -  Monthly h5\n",
    "h5_24h_folder = r'F:\\Documents\\Projects\\ML\\ADCP_ML\\BACUS\\h5_24h_files\\\\'  #Output folder - 24hr h5:\n",
    "\n",
    "\n",
    "from src import convert_monthly_mat_to_h5\n",
    "from src import split_h5_to_24hr_files\n",
    "annotations_file = '' #This is purely classification. No annotation exists. \n",
    "#annotations_file = r'F:\\Documents\\Projects\\ML\\ADCP_ML\\annotations_table_ed05_revised.mat'\n",
    "\n",
    "#PART 1: Convert monthly Mat to monthly h5:\n",
    "for folder in folder_list:\n",
    "    # Path to your .mat file\n",
    "    file_list = os.listdir(data_parent + folder)\n",
    "    mat_files = {k for k in file_list if os.path.splitext(k)[1] == \".mat\"}\n",
    "    print(mat_files)\n",
    "\n",
    "    #Combine the filenames into a proper path:\n",
    "    mat_paths = []\n",
    "    for filename in mat_files:\n",
    "        mat_paths.append(data_parent + folder + '\\\\' + filename) \n",
    "\n",
    "    #Run the extraction\n",
    "    for mat_path in mat_paths:\n",
    "        #print(mat_path)\n",
    "        convert_monthly_mat_to_h5.extract_mat_to_h5(mat_path, h5_monthly_folder) \n",
    "\n",
    "\n",
    "    ######################################\n",
    "    # PART #2: Split to 24 hours, embed annotations and save the time in python format\n",
    "    ######################################\n",
    "        \n",
    "    #Paths to month(ish) HDF5 source file(s):\n",
    "    for mat_path in mat_paths:\n",
    "        filename_mat = os.path.basename(mat_path)\n",
    "        filename_h5 = os.path.splitext(filename_mat)[0] + '.h5'\n",
    "        input_file = h5_monthly_folder + filename_h5\n",
    "\n",
    "        print('Splitting files from folder {}'.format(folder))\n",
    "\n",
    "        split_h5_to_24hr_files.split_h5_to_24hr_files_with_ann(\n",
    "            input_file,             # your big HDF5 source (created with import_monthly_mat_to_h5.py)\n",
    "            h5_24h_folder,          # output dir for 24hr files\n",
    "            annotations_file,        # your .mat annotations file\n",
    "        )\n",
    "\n",
    "#PART 3:\n",
    "\n",
    "#Load, classify, and plot another test file, \n",
    "\n",
    "#LOAD THE TEST FILE\n",
    "file_path = r\"F:\\Documents\\Projects\\ML\\ADCP_ML\\BACUS\\h5_24h_files\\\\\"\n",
    "h5_filename = '20240808T000000_20240808T235959.h5'\n",
    "#\n",
    "h5_test_file = []\n",
    "h5_test_file.append(file_path + h5_filename) \n",
    "\n",
    "#Run the classification\n",
    "x, y, preds, meta = classify_test_data(model, h5_test_file)\n",
    "\n",
    "#Get indices of start/end segments of anomalies\n",
    "annotations = y.view(3,288)\n",
    "predictions = preds.view(3,288)\n",
    "\n",
    "plot_results(x, annotations, predictions, h5_filename, meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Look for Any positives!\n",
    "\n",
    "#Push all files through. If any are classified as drop-outs with more than 6 in a row (half an hour), make a plot\n",
    "\n",
    "#Get a list of files from the directory:\n",
    "#data_folder = \"/scratch/slonimer/ML_ADCP/BACAX_24hr_h5/\"\n",
    "data_folder= r\"F:\\Documents\\Projects\\ML\\ADCP_ML\\BACUS\\h5_24h_files\\\\\"\n",
    "file_list = os.listdir(data_folder)\n",
    "h5_files = {k for k in file_list if os.path.splitext(k)[1] == \".h5\"}\n",
    "#Files are inherently NOT in order in python! So if you want them in order, need to do this:\n",
    "h5_files = sorted(h5_files)  # Sorts alphabetically\n",
    "\n",
    "h5_paths = []\n",
    "for filename in h5_files:\n",
    "    h5_paths.append(data_folder + filename) \n",
    "\n",
    "\n",
    "#Run the classification\n",
    "for file_path in h5_paths:\n",
    "    print(file_path)\n",
    "    #Predict the class\n",
    "    x, y, preds, meta = classify_test_data(model, [file_path])\n",
    "    \n",
    "    #Get indices of start/end segments of anomalies\n",
    "    annotations = y.view(3,288)\n",
    "    predictions = preds.view(3,288)\n",
    "\n",
    "    \n",
    "    #Determine if any annotations present in any beam:\n",
    "    do_plot = 0\n",
    "    for beam in range(3):\n",
    "        ann_beam = annotations[beam].cpu().numpy()\n",
    "        pred_beam = predictions[beam].cpu().numpy()\n",
    "        \n",
    "        #If more than one hour predicted in a day in any beam:\n",
    "        n_samples = 24 # 1 hour\n",
    "        #n_samples = 12 # 1 hour\n",
    "        if np.all(ann_beam==0) and np.sum(pred_beam>0)>n_samples:\n",
    "            do_plot = 1\n",
    "    \n",
    "    if do_plot == 1:\n",
    "        #Plot the results\n",
    "        plot_results(x, annotations, predictions, os.path.basename(file_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['20240701', '20240801', '20240901', '20241001', '20241101', '20241201', '20250101', '20250201', '20250301', '20250401', '20250501']\n"
     ]
    }
   ],
   "source": [
    "data_parent = r'F:\\Documents\\Projects\\ADCP\\scan_for_data\\BACUS\\ADCP2MHZ\\\\'\n",
    "child_folders = os.listdir(data_parent)\n",
    "print(child_folders[-12:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#I moved the code from this cell to \"ADCP_Anomaly_Detection.ipynb\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "science_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
