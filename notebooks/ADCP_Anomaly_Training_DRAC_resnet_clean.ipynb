{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "'''\n",
    "This Notebook is for training an ML model with GPU resources on Digital Research Alliance Canada (DRAC)\n",
    "The data set consists of 4000 h5-format files.  Each file contains 24 hours of data.  \n",
    "\n",
    "Within the h5 files, there is a data group called '/data' with many multi channel variables.  \n",
    "There is also a set of annotations that indicate the classication of the data when anomalies are present.  \n",
    "Generally the class-imbalance for anomalous data is very large.  ( ~1 day with anomalous data per year)\n",
    "\n",
    "Components:\n",
    "1) Custom data loader that loads the data\n",
    "- the data is for an ADCP with 3 beams.  Each beam collects data for beam velocity, beam backscatter, and beam correlation.  \n",
    "- The beams can be thought of as independant data sets, with 3 input channels, so each beam can be fed through the model independantly.  \n",
    "- The \"anomalous\" data may only occur for 6 hours within a 24 hour period, \n",
    "- We want a model to identify specifically which data is anomalous (not just binary yes/no for the entire 24 hour period)\n",
    "\n",
    "2) Define a model architecture / type\n",
    " - This has been designed to be modular, to experiment with differnet options for the architecture \n",
    "\n",
    "3) Loss function \n",
    "- is weighted to account for class-imbalance.  \n",
    "- binary dice loss can be used for single class (true/false)\n",
    "- graduated dice loss can be used for multi-class\n",
    "- also combined with BCE loss for stability in training\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "'''\n",
    "To set up the environment:\n",
    "\n",
    "conda create -n adcp_anomaly_env python=3.10\n",
    "conda activate adcp_anomaly_env\n",
    "pip install -r requirements.txt\n",
    "\n",
    "OR\n",
    "\n",
    "python -m venv adcp_anomaly_env\n",
    "adcp_anomaly_env\\Scripts\\activate\n",
    "pip install -r requirements.txt\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Some changes:\n",
    "# I updated dataset_loader so that MINOR A and MINOR B classes are not included\n",
    "# By this I mean, they are not excluded from training, they just have class = 0 (same as normal data)\n",
    "# I chose to do this because it was super-ceding proper anomalous data, and really skewing the model training\n",
    "# => This change lead to SIGNIFICANT improvement in classification\n",
    "\n",
    "#Session Settings on JupyterHub\n",
    "#Memory: 15000\n",
    "#Cores: 4\n",
    "#GPUs: 1\n",
    "\n",
    "#Kernel: SSAMBA Kernel on Scratch\n",
    "\n",
    "#if DRAC is being buggy, try from ssh:\n",
    "# $ salloc --account=def-kmoran --time=02:00:00 --mem=16G --cpus-per-task=4 --gres=gpu:h100:1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Cell 1: Imports & Setup\n",
    "\n",
    "import os\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "# Add repo root to Python path - Needed to import from src folder\n",
    "import sys\n",
    "from pathlib import Path\n",
    "repo_root = Path().resolve().parent  # notebooks/ → ADCP-CNN-QAQC\n",
    "sys.path.append(str(repo_root))\n",
    "\n",
    "from src.dataset_loader import ADCPDataset  # your custom dataset\n",
    "from src.resnet_temporal import ResNetTemporalClassifier # CNNClassifier  # your model\n",
    "# from model import TemporalCNN # CNNClassifier  # your model\n",
    "from src.utils import seed_everything, get_class_weights, combined_loss, train_model\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "seed_num = 42\n",
    "seed_everything(seed_num)\n",
    "\n",
    "\n",
    "#IMPORTANT NOTE: ALSO NEED TO ADJUST THIS IN \"utils.py\" TO MATCH, in \"def train_model()\"\n",
    "\n",
    "USE_WANDB = False # This is a vestigal implementation from my local computer. I did not have success with WANDB on DRAC resources\n",
    "#USE_WANDB = True  # TODO: Set to True to enable logging - Keep false while in development/debugging\n",
    "if USE_WANDB:\n",
    "    import wandb\n",
    "    \n",
    "DEBUG_MODE = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Initialize Configuration\n",
    "#53842d9970aafed0ab407079e403fe03469dcb33\n",
    "\n",
    "from types import SimpleNamespace\n",
    "\n",
    "USE_WANDB = False # not working on rorqual, so setting to false\n",
    "\n",
    "if USE_WANDB:\n",
    "    wandb.init(project=\"adcp-anomaly-detection\", #dir=\"/scratch/ML_ADCP/wandb_runs\", \n",
    "            config={\n",
    "                \"model\": \"ResNetTemporalClassifier\",\n",
    "                \"epochs\": 100,\n",
    "                \"batch_size\": 16,\n",
    "                \"lr\": 1e-3,\n",
    "                \"loss_alpha\": 0.5,\n",
    "                \"optimizer\": \"Adam\"\n",
    "            })\n",
    "    config = wandb.config\n",
    "else:\n",
    "    config = SimpleNamespace(**{\n",
    "        \"model\": \"ResNetTemporalClassifier\",\n",
    "        \"epochs\": 100,\n",
    "        \"batch_size\": 16, #2,\n",
    "        \"lr\": 1e-3,\n",
    "        \"loss_alpha\": 0.5,\n",
    "        \"optimizer\": \"Adam\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Example: \n",
    "#Tools to reload packages if debugging:\n",
    "import importlib\n",
    "import dataset_loader\n",
    "importlib.reload(dataset_loader)\n",
    "\n",
    "from src.dataset_loader import ADCPDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train/val/test datasets grabbed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/slonimer/torch_env25/lib/python3.11/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Load Dataset\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#Get a list of files from the directory:\n",
    "data_folder = \"/lustre10/scratch/slonimer/BACAX_24hr_h5/\" # Rorqual\n",
    "#data_folder = \"/scratch/slonimer/ML_ADCP/BACAX_24hr_h5/\" # Fir / Cedar\n",
    "#data_folder= r\"F:\\Documents\\Projects\\ML\\ADCP_ML\\h5_24h_files\\\\\" # Local Windows VM\n",
    "\n",
    "file_list = os.listdir(data_folder)\n",
    "h5_files = {k for k in file_list if os.path.splitext(k)[1] == \".h5\"}\n",
    "#Files are inherently NOT in order in python! So need to do this:\n",
    "h5_files = sorted(h5_files)  # Sorts alphabetically\n",
    "#=> This also ensure reproducibility by having a known file order\n",
    "\n",
    "#print(os.path.splitext(file_list[0]))\n",
    "#print(h5_files)\n",
    "\n",
    "h5_paths = []\n",
    "for filename in h5_files:\n",
    "    h5_paths.append(data_folder + filename) \n",
    "\n",
    "#print(len(h5_paths))\n",
    "\n",
    "# Load anomaly filenames from a text file\n",
    "with open(\"annotated_files.txt\", \"r\") as f:\n",
    "    anomaly_files = set(line.strip() for line in f if line.strip())\n",
    "    \n",
    "#Define anomaly paths before truncating h5_paths\n",
    "anomaly_paths = [p for p in h5_paths if os.path.basename(p) in anomaly_files]\n",
    "\n",
    "DEBUG_MODE = 0\n",
    "if DEBUG_MODE:\n",
    "    #WHILE DEBUGGING, ONLY USE A FEW FILES\n",
    "    #NEED THIS FOR DEBUGGING - POTENTIALLY USES 13 GB OF MEMORY\n",
    "    num_files = 200 # 1000 makes the kernel crash w 2400 MB memory\n",
    "    h5_paths = h5_paths[:num_files]    \n",
    "\n",
    "    #file_idx = 3567\n",
    "    #h5_paths = h5_paths[file_idx-5 : file_idx+5 ]                                   \n",
    "\n",
    "#full_dataset = ADCPDataset(h5_paths) # (data_dir)\n",
    "\n",
    "normal_paths = [p for p in h5_paths if os.path.basename(p) not in anomaly_files]\n",
    "\n",
    "\n",
    "# Example: 70% train, 20% val, 10% test\n",
    "#total_size = len(full_dataset)\n",
    "#train_size = int(0.7 * total_size)\n",
    "#val_size = int(0.20 * total_size)\n",
    "#test_size = total_size - train_size - val_size  # ensure all samples are used\n",
    "\n",
    "#train_dataset, val_dataset, test_dataset = random_split(\n",
    "#    full_dataset, [train_size, val_size, test_size]\n",
    "\n",
    "#train_size = int(0.8 * len(full_dataset))\n",
    "#val_size = len(full_dataset) - train_size\n",
    "#train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])\n",
    "\n",
    "# Split anomaly files\n",
    "an_train, an_temp = train_test_split(anomaly_paths, test_size=0.3, random_state=seed_num) #70/30 split\n",
    "an_val, an_test = train_test_split(an_temp, test_size=0.33, random_state=seed_num) #20/10 split \n",
    "\n",
    "# Split normal files\n",
    "n_train, n_temp = train_test_split(h5_paths, test_size=0.3, random_state=seed_num) #70/30 split\n",
    "n_val, n_test = train_test_split(n_temp, test_size=0.33, random_state=seed_num) #20/10 split \n",
    "\n",
    "# Combine\n",
    "train_files = an_train + n_train\n",
    "val_files = an_val + n_val\n",
    "test_files = an_test + n_test\n",
    "\n",
    "#Create the datasets:\n",
    "train_dataset = ADCPDataset(train_files)\n",
    "val_dataset = ADCPDataset(val_files)\n",
    "test_dataset = ADCPDataset(test_files)\n",
    "\n",
    "print('train/val/test datasets grabbed')\n",
    "\n",
    "#Set num_workers to 2x cpu cores (NO, get a warning with that), and pin_memory when using a GPU\n",
    "train_loader = DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "#train_loader = DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True, num_workers=4)\n",
    "val_loader = DataLoader(val_dataset, batch_size=config.batch_size, shuffle=False, num_workers=4)\n",
    "test_loader = DataLoader(test_dataset, batch_size=config.batch_size, shuffle=False, num_workers=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2767\n",
      "795\n",
      "393\n"
     ]
    }
   ],
   "source": [
    "#Example:\n",
    "#if num_files is 20, then len(full_dataset) will be 60 (*3) because of the 3 beams\n",
    "print(len(train_files))\n",
    "print(len(val_files))\n",
    "print(len(test_files))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded pretrained weights with 2 missing and 0 unexpected keys\n",
      "tensor([1.6761e-01, 3.8812e+01, 1.4015e+02, 9.8140e+02, 0.0000e+00, 0.0000e+00])\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Initialize Model and Loss - Resnet50\n",
    "num_classes = 6 # full_dataset.num_classes\n",
    "# model = TemporalCNN(input_channels=3, num_classes=num_classes)\n",
    "\n",
    "variant = 'resnet50'   # or 'resnet18', 'resnet34', etc.\n",
    "model = ResNetTemporalClassifier(\n",
    "    num_classes=num_classes,\n",
    "    pretrained=False,      # set False if you want to train from scratch\n",
    "    variant=variant,     # options: 'resnet50', 'resnet101', 'resnet152' (but only 18,34,50 are avail pre-trained for now)\n",
    "    resize=(224, 224)        # input size for ResNet\n",
    ")\n",
    "#In the \"model\" initialization above, \n",
    "#I've set the pre-trained to false, since no internet, but will load from pre-trained models I got on my VM\n",
    "\n",
    "\n",
    "# --- Load pretrained weights manually (offline) ---\n",
    "pretrained_path = f\"/lustre10/scratch/slonimer/models/{variant}.pth\"\n",
    "state_dict = torch.load(pretrained_path, map_location='cpu')\n",
    "\n",
    "# Filter out the fc layer weights (1000-class classifier)\n",
    "filtered_state_dict = {k: v for k, v in state_dict.items() if not k.startswith(\"fc.\")}\n",
    "\n",
    "# only load matching keys (to ignore classifier layer mismatches)\n",
    "missing, unexpected = model.backbone.load_state_dict(filtered_state_dict, strict=False)\n",
    "print(f\"✅ Loaded pretrained weights with {len(missing)} missing and {len(unexpected)} unexpected keys\")\n",
    "\n",
    "\n",
    "class_weights = get_class_weights(train_dataset, num_classes)                    # FIX THIS LATER -  UNCOMMENT THIS OR DEFINE MANUALLY BUT CORRECT WEIGHTS\n",
    "print(class_weights)\n",
    "#class_weights = torch.tensor([1.6761e-01, 3.8812e+01, 1.4015e+02, 9.8140e+02, 0.0000e+00, 0.0000e+00])\n",
    "\n",
    "loss_fn = combined_loss(class_weights, alpha=config.loss_alpha)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=config.lr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training on Epoch #  0\n",
      "Loss (& total) on Batch #1: 1.519645094871521 (1.519645094871521)\n",
      "Loss (& total) on Batch #2: 1.0346615314483643 (2.5543066263198853)\n",
      "Loss (& total) on Batch #3: 0.5921858549118042 (3.1464924812316895)\n",
      "Loss (& total) on Batch #4: 2.83632230758667 (5.982814788818359)\n",
      "Loss (& total) on Batch #5: 0.576441764831543 (6.559256553649902)\n",
      "Loss (& total) on Batch #6: 0.5786707401275635 (7.137927293777466)\n",
      "Loss (& total) on Batch #7: 0.5579795837402344 (7.6959068775177)\n",
      "Loss (& total) on Batch #8: 0.5194362998008728 (8.215343177318573)\n",
      "Loss (& total) on Batch #9: 0.5143057107925415 (8.729648888111115)\n",
      "Loss (& total) on Batch #10: 0.5095085501670837 (9.239157438278198)\n",
      "Loss (& total) on Batch #11: 0.5061085224151611 (9.74526596069336)\n",
      "Loss (& total) on Batch #12: 0.5171078443527222 (10.262373805046082)\n",
      "Loss (& total) on Batch #13: 0.5030055642127991 (10.76537936925888)\n",
      "Loss (& total) on Batch #14: 0.5043883919715881 (11.269767761230469)\n",
      "Loss (& total) on Batch #15: 0.5030034184455872 (11.772771179676056)\n",
      "Loss (& total) on Batch #16: 0.5028864145278931 (12.275657594203949)\n",
      "Loss (& total) on Batch #17: 0.5012091398239136 (12.776866734027863)\n",
      "Loss (& total) on Batch #18: 5.579020977020264 (18.355887711048126)\n",
      "Loss (& total) on Batch #19: 0.5090473294258118 (18.864935040473938)\n",
      "Loss (& total) on Batch #20: 0.539603590965271 (19.40453863143921)\n",
      "Loss (& total) on Batch #21: 3.264787197113037 (22.669325828552246)\n",
      "Loss (& total) on Batch #22: 0.6761898994445801 (23.345515727996826)\n",
      "Loss (& total) on Batch #23: 0.7810851335525513 (24.126600861549377)\n",
      "Loss (& total) on Batch #24: 0.7912465333938599 (24.917847394943237)\n",
      "Loss (& total) on Batch #25: 1.4932689666748047 (26.411116361618042)\n",
      "Loss (& total) on Batch #26: 0.7710295915603638 (27.182145953178406)\n",
      "Loss (& total) on Batch #27: 0.7929713129997253 (27.97511726617813)\n",
      "Loss (& total) on Batch #28: 0.7096390724182129 (28.684756338596344)\n",
      "Loss (& total) on Batch #29: 1.4304018020629883 (30.115158140659332)\n",
      "Loss (& total) on Batch #30: 0.6632906794548035 (30.778448820114136)\n",
      "Loss (& total) on Batch #31: 0.6387410163879395 (31.417189836502075)\n",
      "Loss (& total) on Batch #32: 0.6164417266845703 (32.033631563186646)\n",
      "Loss (& total) on Batch #33: 0.596625030040741 (32.63025659322739)\n",
      "Loss (& total) on Batch #34: 0.5841371417045593 (33.214393734931946)\n",
      "Loss (& total) on Batch #35: 0.5680533647537231 (33.78244709968567)\n",
      "Loss (& total) on Batch #36: 0.5760020017623901 (34.35844910144806)\n",
      "Loss (& total) on Batch #37: 0.5534277558326721 (34.91187685728073)\n",
      "Loss (& total) on Batch #38: 4.055182933807373 (38.967059791088104)\n",
      "Loss (& total) on Batch #39: 0.5572912693023682 (39.52435106039047)\n",
      "Loss (& total) on Batch #40: 0.5931339859962463 (40.11748504638672)\n",
      "Loss (& total) on Batch #41: 0.6208610534667969 (40.738346099853516)\n",
      "Loss (& total) on Batch #42: 0.6186773180961609 (41.35702341794968)\n",
      "Loss (& total) on Batch #43: 0.6163625121116638 (41.97338593006134)\n",
      "Loss (& total) on Batch #44: 0.6078279614448547 (42.581213891506195)\n",
      "Loss (& total) on Batch #45: 0.6239579319953918 (43.20517182350159)\n",
      "Loss (& total) on Batch #46: 0.5997994542121887 (43.804971277713776)\n",
      "Loss (& total) on Batch #47: 0.5897824764251709 (44.39475375413895)\n",
      "Loss (& total) on Batch #48: 0.5605739951133728 (44.95532774925232)\n",
      "Loss (& total) on Batch #49: 0.5292035341262817 (45.4845312833786)\n",
      "Loss (& total) on Batch #50: 0.5339586138725281 (46.01848989725113)\n",
      "Loss (& total) on Batch #51: 0.5148804783821106 (46.53337037563324)\n",
      "Loss (& total) on Batch #52: 0.5126704573631287 (47.04604083299637)\n",
      "Loss (& total) on Batch #53: 0.5102867484092712 (47.55632758140564)\n",
      "Loss (& total) on Batch #54: 0.5080606341362 (48.06438821554184)\n",
      "Loss (& total) on Batch #55: 0.506102442741394 (48.570490658283234)\n",
      "Loss (& total) on Batch #56: 0.5067657232284546 (49.07725638151169)\n",
      "Loss (& total) on Batch #57: 2.5188913345336914 (51.59614771604538)\n",
      "Loss (& total) on Batch #58: 2.6148078441619873 (54.21095556020737)\n",
      "Loss (& total) on Batch #59: 0.5374414324760437 (54.74839699268341)\n",
      "Loss (& total) on Batch #60: 1.6276166439056396 (56.37601363658905)\n",
      "Loss (& total) on Batch #61: 1.2943999767303467 (57.6704136133194)\n",
      "Loss (& total) on Batch #62: 0.7418334484100342 (58.41224706172943)\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Train\n",
    "\n",
    "best_model_path = f\"best_model_{variant}.pt\"\n",
    "if USE_WANDB:\n",
    "    model = train_model(model, train_loader, val_loader, optimizer, loss_fn, device, num_epochs=config.epochs, patience=5, USE_WANDB=USE_WANDB, best_model_path = best_model_path)\n",
    "else:\n",
    "    model, history = train_model(model, train_loader, val_loader, optimizer, loss_fn, device, num_epochs=config.epochs, patience=5, USE_WANDB=USE_WANDB, best_model_path = best_model_path)\n",
    "\n",
    "# ([2, 3, 288, 102])\n",
    "# => [batch, channels, time, range]\n",
    "    \n",
    "#Best result using ce and dice-loss:\n",
    "#Epoch 7/20 | Train Loss: 0.7003 | Val Loss: 0.9850 | Val F1: 0.4931\n",
    "#⏹️ Early stopping triggered.\n",
    "    \n",
    "# With ce and graduated dice-loss:\n",
    "# Starting Validation on Epoch #  6\n",
    "# Epoch 7/20 | Train Loss: 0.7778 | Val Loss: 0.9043 | Val F1: 0.4931\n",
    "# #\n",
    "# Starting Validation on Epoch #  7\n",
    "# Epoch 8/20 | Train Loss: 0.7598 | Val Loss: 0.9866 | Val F1: 0.4931\n",
    "# ⏹️ Early stopping triggered.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Profile memory from linux terminal with:\n",
    "# $top -u slonimer\n",
    "\n",
    "#8.2 g : Load data loaders\n",
    "#1.4 g: More memory needed for running the model\n",
    "\n",
    "#Notes: Running batch size of 16 for full data set takes a long time.  Should try doing larger batch size.\n",
    "# No 2-power batch sizes are divisible by 3 (number of beams) but could try something larger, like 256\n",
    "\n",
    "\n",
    "# Can also check GPUs:\n",
    "# $ watch -n 1 nvidia-smi\n",
    "\n",
    "# OR CPU utilization\n",
    "# $ ps -u $USER -f | grep pt_data_worker\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.6761e-01, 3.8812e+01, 1.4015e+02, 9.8140e+02, 0.0000e+00, 0.0000e+00])\n",
      "Starting Training on Epoch #  0\n",
      "Loss (& total) on Batch #1: 0.5087339878082275 (0.5087339878082275)\n",
      "Loss (& total) on Batch #2: 0.5012625455856323 (1.0099965333938599)\n",
      "Loss (& total) on Batch #3: 0.5001527667045593 (1.5101493000984192)\n",
      "Loss (& total) on Batch #4: 0.4999542236328125 (2.0101035237312317)\n",
      "Loss (& total) on Batch #5: 0.4999334216117859 (2.5100369453430176)\n",
      "Loss (& total) on Batch #6: 0.4999291002750397 (3.0099660456180573)\n",
      "Loss (& total) on Batch #7: 0.4999285638332367 (3.509894609451294)\n",
      "Loss (& total) on Batch #8: 0.49992835521698 (4.009822964668274)\n",
      "Loss (& total) on Batch #9: 0.49992823600769043 (4.509751200675964)\n",
      "Loss (& total) on Batch #10: 0.4999281167984009 (5.009679317474365)\n",
      "Loss (& total) on Batch #11: 0.4999280571937561 (5.509607374668121)\n",
      "Loss (& total) on Batch #12: 0.49992844462394714 (6.0095358192920685)\n",
      "Loss (& total) on Batch #13: 0.49992841482162476 (6.509464234113693)\n",
      "Loss (& total) on Batch #14: 0.4999280869960785 (7.009392321109772)\n",
      "Loss (& total) on Batch #15: 0.4999281167984009 (7.509320437908173)\n",
      "Loss (& total) on Batch #16: 0.49992796778678894 (8.009248405694962)\n",
      "Loss (& total) on Batch #17: 0.49992790818214417 (8.509176313877106)\n",
      "Loss (& total) on Batch #18: 0.4999278485774994 (9.009104162454605)\n",
      "Loss (& total) on Batch #19: 0.499927818775177 (9.509031981229782)\n",
      "Loss (& total) on Batch #20: 0.4999278485774994 (10.008959829807281)\n",
      "Loss (& total) on Batch #21: 0.4999277889728546 (10.508887618780136)\n",
      "Loss (& total) on Batch #22: 0.4999277889728546 (11.00881540775299)\n",
      "Loss (& total) on Batch #23: 0.4999277889728546 (11.508743196725845)\n",
      "Loss (& total) on Batch #24: 0.4999277889728546 (12.0086709856987)\n",
      "Loss (& total) on Batch #25: 0.4999277889728546 (12.508598774671555)\n",
      "Loss (& total) on Batch #26: 10.070488929748535 (22.57908770442009)\n",
      "Loss (& total) on Batch #27: 21.04482650756836 (43.62391421198845)\n",
      "Loss (& total) on Batch #28: 0.4999684989452362 (44.123882710933685)\n",
      "Loss (& total) on Batch #29: 0.4999667704105377 (44.62384948134422)\n",
      "Loss (& total) on Batch #30: 0.4999634623527527 (45.123812943696976)\n",
      "Loss (& total) on Batch #31: 4.780055046081543 (49.90386798977852)\n",
      "Loss (& total) on Batch #32: 0.5000203251838684 (50.40388831496239)\n",
      "Loss (& total) on Batch #33: 0.5001086592674255 (50.90399697422981)\n",
      "Loss (& total) on Batch #34: 0.5002845525741577 (51.40428152680397)\n",
      "Loss (& total) on Batch #35: 0.5006585717201233 (51.904940098524094)\n",
      "Loss (& total) on Batch #36: 0.501187264919281 (52.406127363443375)\n",
      "Loss (& total) on Batch #37: 0.502923846244812 (52.90905120968819)\n",
      "Loss (& total) on Batch #38: 0.5040891766548157 (53.413140386343)\n",
      "Loss (& total) on Batch #39: 2.176654577255249 (55.58979496359825)\n",
      "Loss (& total) on Batch #40: 0.5140189528465271 (56.10381391644478)\n",
      "Loss (& total) on Batch #41: 0.5120340585708618 (56.61584797501564)\n",
      "Loss (& total) on Batch #42: 0.521530032157898 (57.13737800717354)\n",
      "Loss (& total) on Batch #43: 0.5250996351242065 (57.662477642297745)\n",
      "Loss (& total) on Batch #44: 0.5268616676330566 (58.1893393099308)\n",
      "Loss (& total) on Batch #45: 0.52849942445755 (58.71783873438835)\n",
      "Loss (& total) on Batch #46: 0.531812846660614 (59.249651581048965)\n",
      "Loss (& total) on Batch #47: 0.5333545804023743 (59.78300616145134)\n",
      "Loss (& total) on Batch #48: 1.3942313194274902 (61.17723748087883)\n",
      "Loss (& total) on Batch #49: 0.548317551612854 (61.725555032491684)\n",
      "Loss (& total) on Batch #50: 0.5584524273872375 (62.28400745987892)\n",
      "Loss (& total) on Batch #51: 6.021013259887695 (68.30502071976662)\n",
      "Loss (& total) on Batch #52: 0.5867424607276917 (68.89176318049431)\n",
      "Loss (& total) on Batch #53: 0.6059908270835876 (69.4977540075779)\n",
      "Loss (& total) on Batch #54: 0.6186491250991821 (70.11640313267708)\n",
      "Loss (& total) on Batch #55: 0.6209258437156677 (70.73732897639275)\n",
      "Loss (& total) on Batch #56: 0.6277828812599182 (71.36511185765266)\n",
      "Loss (& total) on Batch #57: 0.6224994659423828 (71.98761132359505)\n",
      "Loss (& total) on Batch #58: 0.6156585216522217 (72.60326984524727)\n",
      "Loss (& total) on Batch #59: 0.6069254279136658 (73.21019527316093)\n",
      "Loss (& total) on Batch #60: 0.5956028699874878 (73.80579814314842)\n",
      "Loss (& total) on Batch #61: 0.5872938632965088 (74.39309200644493)\n",
      "Loss (& total) on Batch #62: 0.5752984285354614 (74.96839043498039)\n",
      "Loss (& total) on Batch #63: 0.5650245547294617 (75.53341498970985)\n",
      "Loss (& total) on Batch #64: 5.131661891937256 (80.66507688164711)\n",
      "Loss (& total) on Batch #65: 0.5511518716812134 (81.21622875332832)\n",
      "Loss (& total) on Batch #66: 0.549734890460968 (81.76596364378929)\n",
      "Loss (& total) on Batch #67: 0.5471892356872559 (82.31315287947655)\n",
      "Loss (& total) on Batch #68: 0.5487712025642395 (82.86192408204079)\n",
      "Loss (& total) on Batch #69: 0.5552149415016174 (83.4171390235424)\n",
      "Loss (& total) on Batch #70: 0.5589190125465393 (83.97605803608894)\n",
      "Loss (& total) on Batch #71: 2.6880764961242676 (86.66413453221321)\n",
      "Loss (& total) on Batch #72: 0.5305029153823853 (87.1946374475956)\n",
      "Loss (& total) on Batch #73: 0.5268810987472534 (87.72151854634285)\n",
      "Loss (& total) on Batch #74: 2.0180916786193848 (89.73961022496223)\n",
      "Loss (& total) on Batch #75: 0.5362027883529663 (90.2758130133152)\n",
      "Loss (& total) on Batch #76: 3.45290470123291 (93.72871771454811)\n",
      "Loss (& total) on Batch #77: 0.586360514163971 (94.31507822871208)\n",
      "Loss (& total) on Batch #78: 0.6401647329330444 (94.95524296164513)\n",
      "Loss (& total) on Batch #79: 0.6353394389152527 (95.59058240056038)\n",
      "Loss (& total) on Batch #80: 0.6334372162818909 (96.22401961684227)\n",
      "Loss (& total) on Batch #81: 0.6511807441711426 (96.87520036101341)\n",
      "Loss (& total) on Batch #82: 1.751988410949707 (98.62718877196312)\n",
      "Loss (& total) on Batch #83: 0.6309661865234375 (99.25815495848656)\n",
      "Loss (& total) on Batch #84: 0.6291792392730713 (99.88733419775963)\n",
      "Loss (& total) on Batch #85: 1.747214436531067 (101.6345486342907)\n",
      "Loss (& total) on Batch #86: 0.6224441528320312 (102.25699278712273)\n",
      "Loss (& total) on Batch #87: 0.6292777061462402 (102.88627049326897)\n",
      "Loss (& total) on Batch #88: 0.632360577583313 (103.51863107085228)\n",
      "Loss (& total) on Batch #89: 0.6321449279785156 (104.1507759988308)\n",
      "Loss (& total) on Batch #90: 0.9287523627281189 (105.07952836155891)\n",
      "Loss (& total) on Batch #91: 0.6286264061927795 (105.7081547677517)\n",
      "Loss (& total) on Batch #92: 1.447674036026001 (107.1558288037777)\n",
      "Loss (& total) on Batch #93: 0.6409425139427185 (107.79677131772041)\n",
      "Loss (& total) on Batch #94: 1.7803065776824951 (109.57707789540291)\n",
      "Loss (& total) on Batch #95: 0.6714167594909668 (110.24849465489388)\n",
      "Loss (& total) on Batch #96: 1.2641522884368896 (111.51264694333076)\n",
      "Loss (& total) on Batch #97: 0.7200272083282471 (112.23267415165901)\n",
      "Loss (& total) on Batch #98: 0.7649878263473511 (112.99766197800636)\n",
      "Loss (& total) on Batch #99: 1.2937953472137451 (114.29145732522011)\n",
      "Loss (& total) on Batch #100: 0.7538341283798218 (115.04529145359993)\n",
      "Loss (& total) on Batch #101: 0.7487455606460571 (115.79403701424599)\n",
      "Loss (& total) on Batch #102: 0.7478946447372437 (116.54193165898323)\n",
      "Loss (& total) on Batch #103: 0.721890389919281 (117.26382204890251)\n",
      "Loss (& total) on Batch #104: 0.7004671096801758 (117.96428915858269)\n",
      "Loss (& total) on Batch #105: 0.6766246557235718 (118.64091381430626)\n",
      "Loss (& total) on Batch #106: 0.6596240401268005 (119.30053785443306)\n",
      "Loss (& total) on Batch #107: 0.6202486753463745 (119.92078652977943)\n",
      "Loss (& total) on Batch #108: 0.7570890784263611 (120.6778756082058)\n",
      "Loss (& total) on Batch #109: 0.5636005997657776 (121.24147620797157)\n",
      "Loss (& total) on Batch #110: 1.1806449890136719 (122.42212119698524)\n",
      "Loss (& total) on Batch #111: 0.5932638049125671 (123.01538500189781)\n",
      "Loss (& total) on Batch #112: 0.5879466533660889 (123.6033316552639)\n",
      "Loss (& total) on Batch #113: 0.594235897064209 (124.19756755232811)\n",
      "Loss (& total) on Batch #114: 0.5909695625305176 (124.78853711485863)\n",
      "Loss (& total) on Batch #115: 0.58307284116745 (125.37160995602608)\n",
      "Loss (& total) on Batch #116: 0.5751370787620544 (125.94674703478813)\n",
      "Loss (& total) on Batch #117: 0.5620170831680298 (126.50876411795616)\n",
      "Loss (& total) on Batch #118: 0.5497773289680481 (127.05854144692421)\n",
      "Loss (& total) on Batch #119: 0.5502332448959351 (127.60877469182014)\n",
      "Loss (& total) on Batch #120: 0.5534219145774841 (128.16219660639763)\n",
      "Loss (& total) on Batch #121: 0.5284099578857422 (128.69060656428337)\n",
      "Loss (& total) on Batch #122: 0.5252853631973267 (129.2158919274807)\n",
      "Loss (& total) on Batch #123: 0.5185152888298035 (129.7344072163105)\n",
      "Loss (& total) on Batch #124: 0.5239506363868713 (130.25835785269737)\n",
      "Loss (& total) on Batch #125: 0.5148616433143616 (130.77321949601173)\n",
      "Loss (& total) on Batch #126: 0.5143240690231323 (131.28754356503487)\n",
      "Loss (& total) on Batch #127: 0.5110645890235901 (131.79860815405846)\n",
      "Loss (& total) on Batch #128: 0.6261132955551147 (132.42472144961357)\n",
      "Loss (& total) on Batch #129: 0.5093047618865967 (132.93402621150017)\n",
      "Loss (& total) on Batch #130: 0.5071201324462891 (133.44114634394646)\n",
      "Loss (& total) on Batch #131: 1.4799712896347046 (134.92111763358116)\n",
      "Loss (& total) on Batch #132: 0.5070599317550659 (135.42817756533623)\n",
      "Loss (& total) on Batch #133: 0.512326180934906 (135.94050374627113)\n",
      "Loss (& total) on Batch #134: 0.5099033117294312 (136.45040705800056)\n",
      "Loss (& total) on Batch #135: 0.514136791229248 (136.9645438492298)\n",
      "Loss (& total) on Batch #136: 0.5138914585113525 (137.47843530774117)\n",
      "Loss (& total) on Batch #137: 0.8056800365447998 (138.28411534428596)\n",
      "Loss (& total) on Batch #138: 0.49568843841552734 (138.7798037827015)\n",
      "Loss (& total) on Batch #139: 0.509585440158844 (139.28938922286034)\n",
      "Loss (& total) on Batch #140: 0.5135238170623779 (139.80291303992271)\n",
      "Loss (& total) on Batch #141: 0.5116263628005981 (140.3145394027233)\n",
      "Loss (& total) on Batch #142: 0.514803409576416 (140.82934281229973)\n",
      "Loss (& total) on Batch #143: 0.5172451734542847 (141.346587985754)\n",
      "Loss (& total) on Batch #144: 0.5162944197654724 (141.86288240551949)\n",
      "Loss (& total) on Batch #145: 0.4975162744522095 (142.3603986799717)\n",
      "Loss (& total) on Batch #146: 0.4969402551651001 (142.8573389351368)\n",
      "Loss (& total) on Batch #147: 0.5141178369522095 (143.371456772089)\n",
      "Loss (& total) on Batch #148: 0.7912541031837463 (144.16271087527275)\n",
      "Loss (& total) on Batch #149: 0.5186082124710083 (144.68131908774376)\n",
      "Loss (& total) on Batch #150: 0.5151992440223694 (145.19651833176613)\n",
      "Loss (& total) on Batch #151: 0.8243566155433655 (146.0208749473095)\n",
      "Loss (& total) on Batch #152: 0.5486880540847778 (146.56956300139427)\n",
      "Loss (& total) on Batch #153: 0.5079635977745056 (147.07752659916878)\n",
      "Loss (& total) on Batch #154: 0.6013020277023315 (147.6788286268711)\n",
      "Loss (& total) on Batch #155: 0.5583139657974243 (148.23714259266853)\n",
      "Loss (& total) on Batch #156: 0.5473570823669434 (148.78449967503548)\n",
      "Loss (& total) on Batch #157: 0.5462735891342163 (149.3307732641697)\n",
      "Loss (& total) on Batch #158: 0.5332750082015991 (149.8640482723713)\n",
      "Loss (& total) on Batch #159: 0.5261666178703308 (150.39021489024162)\n",
      "Loss (& total) on Batch #160: 0.567187488079071 (150.9574023783207)\n",
      "Loss (& total) on Batch #161: 0.5216473340988159 (151.4790497124195)\n",
      "Loss (& total) on Batch #162: 0.5216547846794128 (152.00070449709892)\n",
      "Loss (& total) on Batch #163: 0.5326806902885437 (152.53338518738747)\n",
      "Loss (& total) on Batch #164: 0.5589094161987305 (153.0922946035862)\n",
      "Loss (& total) on Batch #165: 0.5106544494628906 (153.6029490530491)\n",
      "Loss (& total) on Batch #166: 0.5130718946456909 (154.11602094769478)\n",
      "Loss (& total) on Batch #167: 0.512311577796936 (154.62833252549171)\n",
      "Loss (& total) on Batch #168: 0.5121617317199707 (155.14049425721169)\n",
      "Loss (& total) on Batch #169: 0.5275291204452515 (155.66802337765694)\n",
      "Loss (& total) on Batch #170: 0.5099080204963684 (156.1779313981533)\n",
      "Loss (& total) on Batch #171: 0.5160859823226929 (156.694017380476)\n",
      "Loss (& total) on Batch #172: 0.5092818737030029 (157.203299254179)\n",
      "Loss (& total) on Batch #173: 0.5083006024360657 (157.71159985661507)\n",
      "Loss (& total) on Batch #174: 0.5069212913513184 (158.21852114796638)\n",
      "Loss (& total) on Batch #175: 0.5114309787750244 (158.7299521267414)\n",
      "Loss (& total) on Batch #176: 0.5071738958358765 (159.23712602257729)\n",
      "Loss (& total) on Batch #177: 0.5063363313674927 (159.74346235394478)\n",
      "Loss (& total) on Batch #178: 0.5096616148948669 (160.25312396883965)\n",
      "Loss (& total) on Batch #179: 0.5066731572151184 (160.75979712605476)\n",
      "Loss (& total) on Batch #180: 0.5062893033027649 (161.26608642935753)\n",
      "Loss (& total) on Batch #181: 0.5080046653747559 (161.77409109473228)\n",
      "Loss (& total) on Batch #182: 0.5038875937461853 (162.27797868847847)\n",
      "Loss (& total) on Batch #183: 0.49143654108047485 (162.76941522955894)\n",
      "Loss (& total) on Batch #184: 3.602917432785034 (166.37233266234398)\n",
      "Loss (& total) on Batch #185: 0.5050241351127625 (166.87735679745674)\n",
      "Loss (& total) on Batch #186: 0.5047693252563477 (167.3821261227131)\n",
      "Loss (& total) on Batch #187: 0.5051887631416321 (167.88731488585472)\n",
      "Loss (& total) on Batch #188: 3.6939401626586914 (171.5812550485134)\n",
      "Loss (& total) on Batch #189: 0.5064167976379395 (172.08767184615135)\n",
      "Loss (& total) on Batch #190: 0.5093894004821777 (172.59706124663353)\n",
      "Loss (& total) on Batch #191: 0.5099299550056458 (173.10699120163918)\n",
      "Loss (& total) on Batch #192: 0.5236671566963196 (173.6306583583355)\n",
      "Loss (& total) on Batch #193: 0.5122000575065613 (174.14285841584206)\n",
      "Loss (& total) on Batch #194: 0.5166519284248352 (174.6595103442669)\n",
      "Loss (& total) on Batch #195: 0.5166995525360107 (175.1762098968029)\n",
      "Loss (& total) on Batch #196: 0.5181305408477783 (175.69434043765068)\n",
      "Loss (& total) on Batch #197: 0.5196672677993774 (176.21400770545006)\n",
      "Loss (& total) on Batch #198: 0.5179615020751953 (176.73196920752525)\n",
      "Loss (& total) on Batch #199: 0.519426167011261 (177.25139537453651)\n",
      "Loss (& total) on Batch #200: 0.5186260938644409 (177.77002146840096)\n",
      "Loss (& total) on Batch #201: 0.5188709497451782 (178.28889241814613)\n",
      "Loss (& total) on Batch #202: 0.5193933844566345 (178.80828580260277)\n",
      "Loss (& total) on Batch #203: 1.9364938735961914 (180.74477967619896)\n",
      "Loss (& total) on Batch #204: 0.5253486037254333 (181.2701282799244)\n",
      "Loss (& total) on Batch #205: 0.5385491847991943 (181.8086774647236)\n",
      "Loss (& total) on Batch #206: 0.6327827572822571 (182.44146022200584)\n",
      "Loss (& total) on Batch #207: 0.5446696281433105 (182.98612985014915)\n",
      "Loss (& total) on Batch #208: 0.5544434785842896 (183.54057332873344)\n",
      "Loss (& total) on Batch #209: 0.5516731142997742 (184.09224644303322)\n",
      "Loss (& total) on Batch #210: 0.5433078408241272 (184.63555428385735)\n",
      "Loss (& total) on Batch #211: 0.537030816078186 (185.17258509993553)\n",
      "Loss (& total) on Batch #212: 0.5401239991188049 (185.71270909905434)\n",
      "Loss (& total) on Batch #213: 0.5260529518127441 (186.23876205086708)\n",
      "Loss (& total) on Batch #214: 0.5275989174842834 (186.76636096835136)\n",
      "Loss (& total) on Batch #215: 0.5230817794799805 (187.28944274783134)\n",
      "Loss (& total) on Batch #216: 0.5226041674613953 (187.81204691529274)\n",
      "Loss (& total) on Batch #217: 0.5184510350227356 (188.33049795031548)\n",
      "Loss (& total) on Batch #218: 0.520115315914154 (188.85061326622963)\n",
      "Loss (& total) on Batch #219: 0.512876570224762 (189.3634898364544)\n",
      "Loss (& total) on Batch #220: 1.4897196292877197 (190.8532094657421)\n",
      "Loss (& total) on Batch #221: 0.5148342251777649 (191.36804369091988)\n",
      "Loss (& total) on Batch #222: 0.5161254405975342 (191.8841691315174)\n",
      "Loss (& total) on Batch #223: 0.5120809078216553 (192.39625003933907)\n",
      "Loss (& total) on Batch #224: 0.5122770667076111 (192.90852710604668)\n",
      "Loss (& total) on Batch #225: 0.5128234624862671 (193.42135056853294)\n",
      "Loss (& total) on Batch #226: 0.5129441022872925 (193.93429467082024)\n",
      "Loss (& total) on Batch #227: 0.5181810259819031 (194.45247569680214)\n",
      "Loss (& total) on Batch #228: 0.5091924071311951 (194.96166810393333)\n",
      "Loss (& total) on Batch #229: 0.746410608291626 (195.70807871222496)\n",
      "Loss (& total) on Batch #230: 0.5088604092597961 (196.21693912148476)\n",
      "Loss (& total) on Batch #231: 1.0300326347351074 (197.24697175621986)\n",
      "Loss (& total) on Batch #232: 0.5065027475357056 (197.75347450375557)\n",
      "Loss (& total) on Batch #233: 0.5140743255615234 (198.2675488293171)\n",
      "Loss (& total) on Batch #234: 0.5081165432929993 (198.7756653726101)\n",
      "Loss (& total) on Batch #235: 0.5098968148231506 (199.28556218743324)\n",
      "Loss (& total) on Batch #236: 0.5063418745994568 (199.7919040620327)\n",
      "Loss (& total) on Batch #237: 0.8318654298782349 (200.62376949191093)\n",
      "Loss (& total) on Batch #238: 0.5072956681251526 (201.1310651600361)\n",
      "Loss (& total) on Batch #239: 0.6616175174713135 (201.7926826775074)\n",
      "Loss (& total) on Batch #240: 0.5106401443481445 (202.30332282185555)\n",
      "Loss (& total) on Batch #241: 0.5051842331886292 (202.80850705504417)\n",
      "Loss (& total) on Batch #242: 0.507911205291748 (203.31641826033592)\n",
      "Loss (& total) on Batch #243: 0.5143717527389526 (203.83079001307487)\n",
      "Loss (& total) on Batch #244: 0.506445586681366 (204.33723559975624)\n",
      "Loss (& total) on Batch #245: 0.5057673454284668 (204.8430029451847)\n",
      "Loss (& total) on Batch #246: 0.5075291991233826 (205.3505321443081)\n",
      "Loss (& total) on Batch #247: 0.5028800964355469 (205.85341224074364)\n",
      "Loss (& total) on Batch #248: 0.5462937355041504 (206.3997059762478)\n",
      "Loss (& total) on Batch #249: 0.5868503451347351 (206.98655632138252)\n",
      "Loss (& total) on Batch #250: 0.5116617679595947 (207.49821808934212)\n",
      "Loss (& total) on Batch #251: 0.5078696608543396 (208.00608775019646)\n",
      "Loss (& total) on Batch #252: 0.5111770033836365 (208.5172647535801)\n",
      "Loss (& total) on Batch #253: 0.5044504404067993 (209.0217151939869)\n",
      "Loss (& total) on Batch #254: 0.5058271884918213 (209.5275423824787)\n",
      "Loss (& total) on Batch #255: 0.5046963691711426 (210.03223875164986)\n",
      "Loss (& total) on Batch #256: 0.5082187056541443 (210.540457457304)\n",
      "Loss (& total) on Batch #257: 0.5114631056785583 (211.05192056298256)\n",
      "Loss (& total) on Batch #258: 0.5131304264068604 (211.56505098938942)\n",
      "Loss (& total) on Batch #259: 0.5043284893035889 (212.069379478693)\n",
      "Loss (& total) on Batch #260: 0.5027806162834167 (212.57216009497643)\n",
      "Loss (& total) on Batch #261: 0.5073875188827515 (213.07954761385918)\n",
      "Loss (& total) on Batch #262: 0.5030366778373718 (213.58258429169655)\n",
      "Loss (& total) on Batch #263: 0.5028584003448486 (214.0854426920414)\n",
      "Loss (& total) on Batch #264: 0.5044983625411987 (214.5899410545826)\n",
      "Loss (& total) on Batch #265: 0.5029720664024353 (215.09291312098503)\n",
      "Loss (& total) on Batch #266: 0.9205423593521118 (216.01345548033714)\n",
      "Loss (& total) on Batch #267: 0.5079399943351746 (216.52139547467232)\n",
      "Loss (& total) on Batch #268: 0.5958486199378967 (217.11724409461021)\n",
      "Loss (& total) on Batch #269: 1.7368420362472534 (218.85408613085747)\n",
      "Loss (& total) on Batch #270: 0.5683550238609314 (219.4224411547184)\n",
      "Loss (& total) on Batch #271: 0.5246262550354004 (219.9470674097538)\n",
      "Loss (& total) on Batch #272: 0.518038809299469 (220.46510621905327)\n",
      "Loss (& total) on Batch #273: 0.5851051807403564 (221.05021139979362)\n",
      "Loss (& total) on Batch #274: 0.5498685240745544 (221.60007992386818)\n",
      "Loss (& total) on Batch #275: 0.5421386361122131 (222.1422185599804)\n",
      "Loss (& total) on Batch #276: 0.5255148410797119 (222.6677334010601)\n",
      "Loss (& total) on Batch #277: 0.5160230994224548 (223.18375650048256)\n",
      "Loss (& total) on Batch #278: 0.5135720372200012 (223.69732853770256)\n",
      "Loss (& total) on Batch #279: 0.5189114212989807 (224.21623995900154)\n",
      "Loss (& total) on Batch #280: 0.8135175704956055 (225.02975752949715)\n",
      "Loss (& total) on Batch #281: 0.5184342265129089 (225.54819175601006)\n",
      "Loss (& total) on Batch #282: 0.518160343170166 (226.06635209918022)\n",
      "Loss (& total) on Batch #283: 0.5127283334732056 (226.57908043265343)\n",
      "Loss (& total) on Batch #284: 0.5189352631568909 (227.09801569581032)\n",
      "Loss (& total) on Batch #285: 0.5105486512184143 (227.60856434702873)\n",
      "Loss (& total) on Batch #286: 0.5248165130615234 (228.13338086009026)\n",
      "Loss (& total) on Batch #287: 0.5121570229530334 (228.6455378830433)\n",
      "Loss (& total) on Batch #288: 0.5123230218887329 (229.15786090493202)\n",
      "Loss (& total) on Batch #289: 0.5134990811347961 (229.67135998606682)\n",
      "Loss (& total) on Batch #290: 0.5089788436889648 (230.18033882975578)\n",
      "Loss (& total) on Batch #291: 0.5103508830070496 (230.69068971276283)\n",
      "Loss (& total) on Batch #292: 0.5084162354469299 (231.19910594820976)\n",
      "Loss (& total) on Batch #293: 0.5114880800247192 (231.71059402823448)\n",
      "Loss (& total) on Batch #294: 0.5058244466781616 (232.21641847491264)\n",
      "Loss (& total) on Batch #295: 0.5083300471305847 (232.72474852204323)\n",
      "Loss (& total) on Batch #296: 0.5052931308746338 (233.23004165291786)\n",
      "Loss (& total) on Batch #297: 0.5058916211128235 (233.73593327403069)\n",
      "Loss (& total) on Batch #298: 0.505628764629364 (234.24156203866005)\n",
      "Loss (& total) on Batch #299: 0.5055726170539856 (234.74713465571404)\n",
      "Loss (& total) on Batch #300: 0.50522381067276 (235.2523584663868)\n",
      "Loss (& total) on Batch #301: 0.6558609008789062 (235.9082193672657)\n",
      "Loss (& total) on Batch #302: 0.5053611397743225 (236.41358050704002)\n",
      "Loss (& total) on Batch #303: 0.508410632610321 (236.92199113965034)\n",
      "Loss (& total) on Batch #304: 2.5763697624206543 (239.498360902071)\n",
      "Loss (& total) on Batch #305: 0.5056889057159424 (240.00404980778694)\n",
      "Loss (& total) on Batch #306: 0.5118220448493958 (240.51587185263634)\n",
      "Loss (& total) on Batch #307: 0.5116729736328125 (241.02754482626915)\n",
      "Loss (& total) on Batch #308: 0.5150799751281738 (241.54262480139732)\n",
      "Loss (& total) on Batch #309: 0.5159975290298462 (242.05862233042717)\n",
      "Loss (& total) on Batch #310: 0.516917884349823 (242.575540214777)\n",
      "Loss (& total) on Batch #311: 0.518104076385498 (243.0936442911625)\n",
      "Loss (& total) on Batch #312: 0.5277307629585266 (243.62137505412102)\n",
      "Loss (& total) on Batch #313: 0.531995952129364 (244.15337100625038)\n",
      "Loss (& total) on Batch #314: 0.5253379344940186 (244.6787089407444)\n",
      "Loss (& total) on Batch #315: 0.5286450982093811 (245.20735403895378)\n",
      "Loss (& total) on Batch #316: 0.5275862216949463 (245.73494026064873)\n",
      "Loss (& total) on Batch #317: 0.7110315561294556 (246.44597181677818)\n",
      "Loss (& total) on Batch #318: 0.7590929269790649 (247.20506474375725)\n",
      "Loss (& total) on Batch #319: 0.522846519947052 (247.7279112637043)\n",
      "Loss (& total) on Batch #320: 0.53662109375 (248.2645323574543)\n",
      "Loss (& total) on Batch #321: 0.5383632779121399 (248.80289563536644)\n",
      "Loss (& total) on Batch #322: 0.5215797424316406 (249.32447537779808)\n",
      "Loss (& total) on Batch #323: 0.5192389488220215 (249.8437143266201)\n",
      "Loss (& total) on Batch #324: 0.5196830630302429 (250.36339738965034)\n",
      "Loss (& total) on Batch #325: 0.6177746057510376 (250.98117199540138)\n",
      "Loss (& total) on Batch #326: 0.5141211748123169 (251.4952931702137)\n",
      "Loss (& total) on Batch #327: 0.5167450904846191 (252.01203826069832)\n",
      "Loss (& total) on Batch #328: 0.5146299600601196 (252.52666822075844)\n",
      "Loss (& total) on Batch #329: 0.514606773853302 (253.04127499461174)\n",
      "Loss (& total) on Batch #330: 0.5153387784957886 (253.55661377310753)\n",
      "Loss (& total) on Batch #331: 0.5094999670982361 (254.06611374020576)\n",
      "Loss (& total) on Batch #332: 0.5092200040817261 (254.5753337442875)\n",
      "Loss (& total) on Batch #333: 0.5089551210403442 (255.08428886532784)\n",
      "Loss (& total) on Batch #334: 0.5091758370399475 (255.59346470236778)\n",
      "Loss (& total) on Batch #335: 0.5085524320602417 (256.102017134428)\n",
      "Loss (& total) on Batch #336: 0.5074105262756348 (256.60942766070366)\n",
      "Loss (& total) on Batch #337: 0.5076967477798462 (257.1171244084835)\n",
      "Loss (& total) on Batch #338: 0.5098099112510681 (257.6269343197346)\n",
      "Loss (& total) on Batch #339: 0.5085000395774841 (258.13543435931206)\n",
      "Loss (& total) on Batch #340: 0.5094091296195984 (258.64484348893166)\n",
      "Loss (& total) on Batch #341: 0.5054717063903809 (259.15031519532204)\n",
      "Loss (& total) on Batch #342: 0.6178467273712158 (259.76816192269325)\n",
      "Loss (& total) on Batch #343: 0.5075520873069763 (260.27571401000023)\n",
      "Loss (& total) on Batch #344: 0.5064777135848999 (260.78219172358513)\n",
      "Loss (& total) on Batch #345: 0.5082873106002808 (261.2904790341854)\n",
      "Loss (& total) on Batch #346: 0.5060511827468872 (261.7965302169323)\n",
      "Loss (& total) on Batch #347: 0.5064579844474792 (262.3029882013798)\n",
      "Loss (& total) on Batch #348: 0.5082831978797913 (262.81127139925957)\n",
      "Loss (& total) on Batch #349: 0.503407895565033 (263.3146792948246)\n",
      "Loss (& total) on Batch #350: 0.666624128818512 (263.9813034236431)\n",
      "Loss (& total) on Batch #351: 0.575008749961853 (264.55631217360497)\n",
      "Loss (& total) on Batch #352: 0.518410325050354 (265.0747224986553)\n",
      "Loss (& total) on Batch #353: 0.5058600306510925 (265.5805825293064)\n",
      "Loss (& total) on Batch #354: 0.5049724578857422 (266.08555498719215)\n",
      "Loss (& total) on Batch #355: 0.5096114277839661 (266.5951664149761)\n",
      "Loss (& total) on Batch #356: 0.5097255706787109 (267.10489198565483)\n",
      "Loss (& total) on Batch #357: 0.50934237241745 (267.6142343580723)\n",
      "Loss (& total) on Batch #358: 0.5057575106620789 (268.11999186873436)\n",
      "Loss (& total) on Batch #359: 0.5092973113059998 (268.62928918004036)\n",
      "Loss (& total) on Batch #360: 0.5064849853515625 (269.1357741653919)\n",
      "Loss (& total) on Batch #361: 0.5065507888793945 (269.6423249542713)\n",
      "Loss (& total) on Batch #362: 0.5058889985084534 (270.14821395277977)\n",
      "Loss (& total) on Batch #363: 0.5032398700714111 (270.6514538228512)\n",
      "Loss (& total) on Batch #364: 0.5039708614349365 (271.1554246842861)\n",
      "Loss (& total) on Batch #365: 0.5035867094993591 (271.6590113937855)\n",
      "Loss (& total) on Batch #366: 0.5122711062431335 (272.1712825000286)\n",
      "Loss (& total) on Batch #367: 4.800643444061279 (276.9719259440899)\n",
      "Loss (& total) on Batch #368: 0.5120735764503479 (277.48399952054024)\n",
      "Loss (& total) on Batch #369: 0.5182422399520874 (278.0022417604923)\n",
      "Loss (& total) on Batch #370: 0.6084805130958557 (278.6107222735882)\n",
      "Loss (& total) on Batch #371: 0.6690759658813477 (279.2797982394695)\n",
      "Loss (& total) on Batch #372: 0.7406716346740723 (280.0204698741436)\n",
      "Loss (& total) on Batch #373: 0.7825952768325806 (280.8030651509762)\n",
      "Loss (& total) on Batch #374: 0.7002583146095276 (281.5033234655857)\n",
      "Loss (& total) on Batch #375: 1.670902132987976 (283.1742255985737)\n",
      "Loss (& total) on Batch #376: 0.5946472883224487 (283.76887288689613)\n",
      "Loss (& total) on Batch #377: 0.5909839272499084 (284.35985681414604)\n",
      "Loss (& total) on Batch #378: 0.6139068603515625 (284.9737636744976)\n",
      "Loss (& total) on Batch #379: 0.5791712403297424 (285.55293491482735)\n",
      "Loss (& total) on Batch #380: 0.5711186528205872 (286.12405356764793)\n",
      "Loss (& total) on Batch #381: 0.5622367262840271 (286.68629029393196)\n",
      "Loss (& total) on Batch #382: 1.6411288976669312 (288.3274191915989)\n",
      "Loss (& total) on Batch #383: 0.5586265325546265 (288.8860457241535)\n",
      "Loss (& total) on Batch #384: 0.5563037991523743 (289.4423495233059)\n",
      "Loss (& total) on Batch #385: 0.5632236003875732 (290.00557312369347)\n",
      "Loss (& total) on Batch #386: 0.5596096515655518 (290.565182775259)\n",
      "Loss (& total) on Batch #387: 0.5532947778701782 (291.1184775531292)\n",
      "Loss (& total) on Batch #388: 0.5539576411247253 (291.6724351942539)\n",
      "Loss (& total) on Batch #389: 1.53395414352417 (293.2063893377781)\n",
      "Loss (& total) on Batch #390: 0.5518400073051453 (293.75822934508324)\n",
      "Loss (& total) on Batch #391: 0.5596843361854553 (294.3179136812687)\n",
      "Loss (& total) on Batch #392: 0.565478503704071 (294.88339218497276)\n",
      "Loss (& total) on Batch #393: 0.5727289915084839 (295.45612117648125)\n",
      "Loss (& total) on Batch #394: 0.5608868598937988 (296.01700803637505)\n",
      "Loss (& total) on Batch #395: 0.5666459798812866 (296.58365401625633)\n",
      "Loss (& total) on Batch #396: 0.5630884170532227 (297.14674243330956)\n",
      "Loss (& total) on Batch #397: 0.5526059865951538 (297.6993484199047)\n",
      "Loss (& total) on Batch #398: 0.5439401268959045 (298.2432885468006)\n",
      "Loss (& total) on Batch #399: 0.5355851650238037 (298.7788737118244)\n",
      "Loss (& total) on Batch #400: 0.5326912999153137 (299.31156501173973)\n",
      "Loss (& total) on Batch #401: 0.5339615345001221 (299.84552654623985)\n",
      "Loss (& total) on Batch #402: 0.523655354976654 (300.3691819012165)\n",
      "Loss (& total) on Batch #403: 0.5233678221702576 (300.89254972338676)\n",
      "Loss (& total) on Batch #404: 0.5166695713996887 (301.40921929478645)\n",
      "Loss (& total) on Batch #405: 0.513855516910553 (301.923074811697)\n",
      "Loss (& total) on Batch #406: 0.5108360648155212 (302.4339108765125)\n",
      "Loss (& total) on Batch #407: 0.5115858316421509 (302.9454967081547)\n",
      "Loss (& total) on Batch #408: 0.5072774887084961 (303.4527741968632)\n",
      "Loss (& total) on Batch #409: 1.1192630529403687 (304.57203724980354)\n",
      "Loss (& total) on Batch #410: 0.5059968829154968 (305.07803413271904)\n",
      "Loss (& total) on Batch #411: 0.5075621008872986 (305.58559623360634)\n",
      "Loss (& total) on Batch #412: 0.7632952928543091 (306.34889152646065)\n",
      "Loss (& total) on Batch #413: 0.5073758363723755 (306.856267362833)\n",
      "Loss (& total) on Batch #414: 0.5084033608436584 (307.3646707236767)\n",
      "Loss (& total) on Batch #415: 0.5084523558616638 (307.87312307953835)\n",
      "Loss (& total) on Batch #416: 0.715783953666687 (308.58890703320503)\n",
      "Loss (& total) on Batch #417: 0.5063498616218567 (309.0952568948269)\n",
      "Loss (& total) on Batch #418: 0.5075244903564453 (309.60278138518333)\n",
      "Loss (& total) on Batch #419: 0.507819652557373 (310.1106010377407)\n",
      "Loss (& total) on Batch #420: 0.5016457438468933 (310.6122467815876)\n",
      "Loss (& total) on Batch #421: 0.5095301866531372 (311.12177696824074)\n",
      "Loss (& total) on Batch #422: 0.504673957824707 (311.62645092606544)\n",
      "Loss (& total) on Batch #423: 0.5134817957878113 (312.13993272185326)\n",
      "Loss (& total) on Batch #424: 0.5143957138061523 (312.6543284356594)\n",
      "Loss (& total) on Batch #425: 0.5245549082756042 (313.178883343935)\n",
      "Loss (& total) on Batch #426: 0.5134987235069275 (313.69238206744194)\n",
      "Loss (& total) on Batch #427: 0.5203953385353088 (314.21277740597725)\n",
      "Loss (& total) on Batch #428: 0.5037258267402649 (314.7165032327175)\n",
      "Loss (& total) on Batch #429: 0.5057833194732666 (315.2222865521908)\n",
      "Loss (& total) on Batch #430: 0.5300458073616028 (315.7523323595524)\n",
      "Loss (& total) on Batch #431: 0.5047763586044312 (316.2571087181568)\n",
      "Loss (& total) on Batch #432: 0.5032253265380859 (316.7603340446949)\n",
      "Loss (& total) on Batch #433: 0.5049925446510315 (317.26532658934593)\n",
      "Loss (& total) on Batch #434: 0.5052709579467773 (317.7705975472927)\n",
      "Loss (& total) on Batch #435: 0.504863977432251 (318.27546152472496)\n",
      "Loss (& total) on Batch #436: 0.4914657175540924 (318.76692724227905)\n",
      "Loss (& total) on Batch #437: 0.5038654804229736 (319.270792722702)\n",
      "Loss (& total) on Batch #438: 0.5075803399085999 (319.7783730626106)\n",
      "Loss (& total) on Batch #439: 0.5024809241294861 (320.2808539867401)\n",
      "Loss (& total) on Batch #440: 0.5040103793144226 (320.78486436605453)\n",
      "Loss (& total) on Batch #441: 0.5024583339691162 (321.28732270002365)\n",
      "Loss (& total) on Batch #442: 0.5054308176040649 (321.7927535176277)\n",
      "Loss (& total) on Batch #443: 0.5022619366645813 (322.2950154542923)\n",
      "Loss (& total) on Batch #444: 0.5019018054008484 (322.79691725969315)\n",
      "Loss (& total) on Batch #445: 0.5023404955863953 (323.29925775527954)\n",
      "Loss (& total) on Batch #446: 0.5005260705947876 (323.79978382587433)\n",
      "Loss (& total) on Batch #447: 0.5021982789039612 (324.3019821047783)\n",
      "Loss (& total) on Batch #448: 0.9073164463043213 (325.2092985510826)\n",
      "Loss (& total) on Batch #449: 0.5137747526168823 (325.7230733036995)\n",
      "Loss (& total) on Batch #450: 0.5093644857406616 (326.23243778944016)\n",
      "Loss (& total) on Batch #451: 0.5026357173919678 (326.7350735068321)\n",
      "Loss (& total) on Batch #452: 0.503589391708374 (327.2386628985405)\n",
      "Loss (& total) on Batch #453: 0.5035892724990845 (327.7422521710396)\n",
      "Loss (& total) on Batch #454: 0.5019592046737671 (328.24421137571335)\n",
      "Loss (& total) on Batch #455: 0.5029755234718323 (328.7471868991852)\n",
      "Loss (& total) on Batch #456: 0.6269564032554626 (329.37414330244064)\n",
      "Loss (& total) on Batch #457: 0.5052269101142883 (329.87937021255493)\n",
      "Loss (& total) on Batch #458: 0.5094729065895081 (330.38884311914444)\n",
      "Loss (& total) on Batch #459: 0.5037785172462463 (330.8926216363907)\n",
      "Loss (& total) on Batch #460: 0.5067873001098633 (331.39940893650055)\n",
      "Loss (& total) on Batch #461: 0.5045050978660583 (331.9039140343666)\n",
      "Loss (& total) on Batch #462: 0.52066969871521 (332.4245837330818)\n",
      "Loss (& total) on Batch #463: 0.5051183104515076 (332.9297020435333)\n",
      "Loss (& total) on Batch #464: 0.5116145610809326 (333.44131660461426)\n",
      "Loss (& total) on Batch #465: 0.518024206161499 (333.95934081077576)\n",
      "Loss (& total) on Batch #466: 0.5070158243179321 (334.4663566350937)\n",
      "Loss (& total) on Batch #467: 0.5043573975563049 (334.97071403265)\n",
      "Loss (& total) on Batch #468: 0.505617082118988 (335.476331114769)\n",
      "Loss (& total) on Batch #469: 0.5197543501853943 (335.9960854649544)\n",
      "Loss (& total) on Batch #470: 0.5433056354522705 (336.53939110040665)\n",
      "Loss (& total) on Batch #471: 0.5032860636711121 (337.04267716407776)\n",
      "Loss (& total) on Batch #472: 0.5056661367416382 (337.5483433008194)\n",
      "Loss (& total) on Batch #473: 0.5033196210861206 (338.0516629219055)\n",
      "Loss (& total) on Batch #474: 0.505100667476654 (338.5567635893822)\n",
      "Loss (& total) on Batch #475: 0.5044125914573669 (339.06117618083954)\n",
      "Loss (& total) on Batch #476: 0.5060437917709351 (339.5672199726105)\n",
      "Loss (& total) on Batch #477: 0.5127325057983398 (340.0799524784088)\n",
      "Loss (& total) on Batch #478: 0.5011553764343262 (340.58110785484314)\n",
      "Loss (& total) on Batch #479: 0.5010268092155457 (341.0821346640587)\n",
      "Loss (& total) on Batch #480: 0.5016278624534607 (341.58376252651215)\n",
      "Loss (& total) on Batch #481: 0.5144919753074646 (342.0982545018196)\n",
      "Loss (& total) on Batch #482: 2.143446922302246 (344.24170142412186)\n",
      "Loss (& total) on Batch #483: 0.5021977424621582 (344.743899166584)\n",
      "Loss (& total) on Batch #484: 0.5017780065536499 (345.24567717313766)\n",
      "Loss (& total) on Batch #485: 0.5056910514831543 (345.7513682246208)\n",
      "Loss (& total) on Batch #486: 1.1244038343429565 (346.8757720589638)\n",
      "Loss (& total) on Batch #487: 0.5982969999313354 (347.4740690588951)\n",
      "Loss (& total) on Batch #488: 0.5084507465362549 (347.98251980543137)\n",
      "Loss (& total) on Batch #489: 0.5115737915039062 (348.4940935969353)\n",
      "Loss (& total) on Batch #490: 0.524956464767456 (349.0190500617027)\n",
      "Loss (& total) on Batch #491: 0.5368958711624146 (349.55594593286514)\n",
      "Loss (& total) on Batch #492: 0.6396957039833069 (350.19564163684845)\n",
      "Loss (& total) on Batch #493: 0.5794734954833984 (350.77511513233185)\n",
      "Loss (& total) on Batch #494: 0.5360360741615295 (351.3111512064934)\n",
      "Loss (& total) on Batch #495: 0.5754633545875549 (351.88661456108093)\n",
      "Loss (& total) on Batch #496: 0.5134924054145813 (352.4001069664955)\n",
      "Loss (& total) on Batch #497: 0.5195345878601074 (352.9196415543556)\n",
      "Loss (& total) on Batch #498: 0.5215194821357727 (353.4411610364914)\n",
      "Loss (& total) on Batch #499: 0.5084683895111084 (353.9496294260025)\n",
      "Loss (& total) on Batch #500: 0.5091552734375 (354.45878469944)\n",
      "Loss (& total) on Batch #501: 0.506912887096405 (354.9656975865364)\n",
      "Loss (& total) on Batch #502: 0.5044757723808289 (355.47017335891724)\n",
      "Loss (& total) on Batch #503: 0.5049660801887512 (355.975139439106)\n",
      "Loss (& total) on Batch #504: 0.5036112070083618 (356.47875064611435)\n",
      "Loss (& total) on Batch #505: 0.5017577409744263 (356.9805083870888)\n",
      "Loss (& total) on Batch #506: 0.5034204721450806 (357.48392885923386)\n",
      "Loss (& total) on Batch #507: 0.5026119947433472 (357.9865408539772)\n",
      "Loss (& total) on Batch #508: 0.5031118988990784 (358.4896527528763)\n",
      "Loss (& total) on Batch #509: 0.5042312145233154 (358.9938839673996)\n",
      "Loss (& total) on Batch #510: 2.3279645442962646 (361.32184851169586)\n",
      "Loss (& total) on Batch #511: 0.504893958568573 (361.82674247026443)\n",
      "Loss (& total) on Batch #512: 0.5043826103210449 (362.3311250805855)\n",
      "Loss (& total) on Batch #513: 0.5054600238800049 (362.8365851044655)\n",
      "Loss (& total) on Batch #514: 0.5074881911277771 (363.34407329559326)\n",
      "Loss (& total) on Batch #515: 0.5094282627105713 (363.85350155830383)\n",
      "Loss (& total) on Batch #516: 0.5113797783851624 (364.364881336689)\n",
      "Loss (& total) on Batch #517: 2.256722927093506 (366.6216042637825)\n",
      "Loss (& total) on Batch #518: 0.5210858583450317 (367.14269012212753)\n",
      "Loss (& total) on Batch #519: 0.5298015475273132 (367.67249166965485)\n",
      "Starting Validation on Epoch #  0\n",
      "Epoch 1/100 | Train Loss: 0.7084 | Val Loss: 0.5606 | Val F1: 0.4312\n",
      "✅ New best model saved.\n",
      "Starting Training on Epoch #  1\n",
      "Loss (& total) on Batch #1: 0.5382921099662781 (0.5382921099662781)\n",
      "Loss (& total) on Batch #2: 0.5422618985176086 (1.0805540084838867)\n",
      "Loss (& total) on Batch #3: 0.5453730821609497 (1.6259270906448364)\n",
      "Loss (& total) on Batch #4: 0.5571587681770325 (2.183085858821869)\n",
      "Loss (& total) on Batch #5: 0.5680226683616638 (2.7511085271835327)\n",
      "Loss (& total) on Batch #6: 0.5630322098731995 (3.314140737056732)\n",
      "Loss (& total) on Batch #7: 0.5607101917266846 (3.8748509287834167)\n",
      "Loss (& total) on Batch #8: 0.5863572359085083 (4.461208164691925)\n",
      "Loss (& total) on Batch #9: 0.5634818077087402 (5.024689972400665)\n",
      "Loss (& total) on Batch #10: 0.5645216703414917 (5.589211642742157)\n",
      "Loss (& total) on Batch #11: 0.5636233687400818 (6.152835011482239)\n",
      "Loss (& total) on Batch #12: 0.5497275590896606 (6.702562570571899)\n",
      "Loss (& total) on Batch #13: 0.549961268901825 (7.252523839473724)\n",
      "Loss (& total) on Batch #14: 0.5544843077659607 (7.807008147239685)\n",
      "Loss (& total) on Batch #15: 0.5426476001739502 (8.349655747413635)\n",
      "Loss (& total) on Batch #16: 0.5397844910621643 (8.8894402384758)\n",
      "Loss (& total) on Batch #17: 0.5309310555458069 (9.420371294021606)\n",
      "Loss (& total) on Batch #18: 0.5326427221298218 (9.953014016151428)\n",
      "Loss (& total) on Batch #19: 0.524910569190979 (10.477924585342407)\n",
      "Loss (& total) on Batch #20: 0.5219256281852722 (10.99985021352768)\n",
      "Loss (& total) on Batch #21: 0.516715407371521 (11.5165656208992)\n",
      "Loss (& total) on Batch #22: 0.5147283673286438 (12.031293988227844)\n",
      "Loss (& total) on Batch #23: 0.5138922929763794 (12.545186281204224)\n",
      "Loss (& total) on Batch #24: 0.5194748640060425 (13.064661145210266)\n",
      "Loss (& total) on Batch #25: 0.5129218101501465 (13.577582955360413)\n",
      "Loss (& total) on Batch #26: 0.5107057690620422 (14.088288724422455)\n",
      "Loss (& total) on Batch #27: 0.5118774175643921 (14.600166141986847)\n",
      "Loss (& total) on Batch #28: 0.7650406360626221 (15.365206778049469)\n",
      "Loss (& total) on Batch #29: 0.5061275362968445 (15.871334314346313)\n",
      "Loss (& total) on Batch #30: 0.5085432529449463 (16.37987756729126)\n",
      "Loss (& total) on Batch #31: 0.5054020881652832 (16.885279655456543)\n",
      "Loss (& total) on Batch #32: 0.5055837035179138 (17.390863358974457)\n",
      "Loss (& total) on Batch #33: 0.5236799120903015 (17.91454327106476)\n",
      "Loss (& total) on Batch #34: 0.9924046993255615 (18.90694797039032)\n",
      "Loss (& total) on Batch #35: 0.5070772171020508 (19.41402518749237)\n",
      "Loss (& total) on Batch #36: 0.6313953399658203 (20.04542052745819)\n",
      "Loss (& total) on Batch #37: 0.5066403746604919 (20.552060902118683)\n",
      "Loss (& total) on Batch #38: 0.5048041343688965 (21.05686503648758)\n",
      "Loss (& total) on Batch #39: 0.531002938747406 (21.587867975234985)\n",
      "Loss (& total) on Batch #40: 0.5047912001609802 (22.092659175395966)\n",
      "Loss (& total) on Batch #41: 0.5089208483695984 (22.601580023765564)\n",
      "Loss (& total) on Batch #42: 0.5080092549324036 (23.109589278697968)\n",
      "Loss (& total) on Batch #43: 0.507534384727478 (23.617123663425446)\n",
      "Loss (& total) on Batch #44: 0.5051177740097046 (24.12224143743515)\n",
      "Loss (& total) on Batch #45: 0.5053671002388 (24.62760853767395)\n",
      "Loss (& total) on Batch #46: 0.7063742876052856 (25.333982825279236)\n",
      "Loss (& total) on Batch #47: 0.5055539011955261 (25.839536726474762)\n",
      "Loss (& total) on Batch #48: 0.505785346031189 (26.34532207250595)\n",
      "Loss (& total) on Batch #49: 0.5110935568809509 (26.856415629386902)\n",
      "Loss (& total) on Batch #50: 0.5055248141288757 (27.361940443515778)\n",
      "Loss (& total) on Batch #51: 0.5105917453765869 (27.872532188892365)\n",
      "Loss (& total) on Batch #52: 0.5050333142280579 (28.377565503120422)\n",
      "Loss (& total) on Batch #53: 1.111701488494873 (29.489266991615295)\n",
      "Loss (& total) on Batch #54: 0.5058929324150085 (29.995159924030304)\n",
      "Loss (& total) on Batch #55: 0.5046041011810303 (30.499764025211334)\n",
      "Loss (& total) on Batch #56: 0.5052364468574524 (31.005000472068787)\n",
      "Loss (& total) on Batch #57: 0.5039489269256592 (31.508949398994446)\n",
      "Loss (& total) on Batch #58: 0.5050029754638672 (32.01395237445831)\n",
      "Loss (& total) on Batch #59: 0.5064650177955627 (32.520417392253876)\n",
      "Loss (& total) on Batch #60: 0.5046650767326355 (33.02508246898651)\n",
      "Loss (& total) on Batch #61: 0.5055058002471924 (33.530588269233704)\n",
      "Loss (& total) on Batch #62: 0.5178898572921753 (34.04847812652588)\n",
      "Loss (& total) on Batch #63: 1.393139123916626 (35.441617250442505)\n",
      "Loss (& total) on Batch #64: 0.5048649311065674 (35.94648218154907)\n",
      "Loss (& total) on Batch #65: 0.5098602771759033 (36.456342458724976)\n",
      "Loss (& total) on Batch #66: 2.762875556945801 (39.219218015670776)\n",
      "Loss (& total) on Batch #67: 0.5165780782699585 (39.735796093940735)\n",
      "Loss (& total) on Batch #68: 0.670490026473999 (40.406286120414734)\n",
      "Loss (& total) on Batch #69: 0.5281358957290649 (40.9344220161438)\n",
      "Loss (& total) on Batch #70: 0.5274097919464111 (41.46183180809021)\n",
      "Loss (& total) on Batch #71: 0.5349761843681335 (41.99680799245834)\n",
      "Loss (& total) on Batch #72: 0.5515735745429993 (42.54838156700134)\n",
      "Loss (& total) on Batch #73: 0.5473758578300476 (43.09575742483139)\n",
      "Loss (& total) on Batch #74: 0.5567762851715088 (43.6525337100029)\n",
      "Loss (& total) on Batch #75: 0.5690428614616394 (44.22157657146454)\n",
      "Loss (& total) on Batch #76: 0.5591604709625244 (44.78073704242706)\n",
      "Loss (& total) on Batch #77: 0.554701030254364 (45.33543807268143)\n",
      "Loss (& total) on Batch #78: 0.5558241605758667 (45.891262233257294)\n",
      "Loss (& total) on Batch #79: 0.5954121351242065 (46.4866743683815)\n",
      "Loss (& total) on Batch #80: 0.5471863746643066 (47.03386074304581)\n",
      "Loss (& total) on Batch #81: 0.5401706695556641 (47.57403141260147)\n",
      "Loss (& total) on Batch #82: 0.538806140422821 (48.11283755302429)\n",
      "Loss (& total) on Batch #83: 0.5378811359405518 (48.650718688964844)\n",
      "Loss (& total) on Batch #84: 0.5316208600997925 (49.182339549064636)\n",
      "Loss (& total) on Batch #85: 0.5426396727561951 (49.72497922182083)\n",
      "Loss (& total) on Batch #86: 0.5272420644760132 (50.252221286296844)\n",
      "Loss (& total) on Batch #87: 0.5245233178138733 (50.77674460411072)\n",
      "Loss (& total) on Batch #88: 0.5267326831817627 (51.30347728729248)\n",
      "Loss (& total) on Batch #89: 1.0026463270187378 (52.30612361431122)\n",
      "Loss (& total) on Batch #90: 0.5200217366218567 (52.826145350933075)\n",
      "Loss (& total) on Batch #91: 0.5187193751335144 (53.34486472606659)\n",
      "Loss (& total) on Batch #92: 0.5145877599716187 (53.85945248603821)\n",
      "Loss (& total) on Batch #93: 0.5200511813163757 (54.379503667354584)\n",
      "Loss (& total) on Batch #94: 0.5229030251502991 (54.90240669250488)\n",
      "Loss (& total) on Batch #95: 0.5138761401176453 (55.41628283262253)\n",
      "Loss (& total) on Batch #96: 0.5166639089584351 (55.93294674158096)\n",
      "Loss (& total) on Batch #97: 0.5130850672721863 (56.44603180885315)\n",
      "Loss (& total) on Batch #98: 0.5143001079559326 (56.96033191680908)\n",
      "Loss (& total) on Batch #99: 0.5127294063568115 (57.473061323165894)\n",
      "Loss (& total) on Batch #100: 0.5298076868057251 (58.00286900997162)\n",
      "Loss (& total) on Batch #101: 0.5252104997634888 (58.52807950973511)\n",
      "Loss (& total) on Batch #102: 0.5096749663352966 (59.037754476070404)\n",
      "Loss (& total) on Batch #103: 0.5114766955375671 (59.54923117160797)\n",
      "Loss (& total) on Batch #104: 0.7531293034553528 (60.302360475063324)\n",
      "Loss (& total) on Batch #105: 0.5100400447845459 (60.81240051984787)\n",
      "Loss (& total) on Batch #106: 0.5084826946258545 (61.320883214473724)\n",
      "Loss (& total) on Batch #107: 0.506883442401886 (61.82776665687561)\n",
      "Loss (& total) on Batch #108: 2.1571085453033447 (63.984875202178955)\n",
      "Loss (& total) on Batch #109: 0.5104225277900696 (64.49529772996902)\n",
      "Loss (& total) on Batch #110: 0.5107471942901611 (65.00604492425919)\n",
      "Loss (& total) on Batch #111: 0.5134481191635132 (65.5194930434227)\n",
      "Loss (& total) on Batch #112: 0.5138275623321533 (66.03332060575485)\n",
      "Loss (& total) on Batch #113: 0.5244020223617554 (66.55772262811661)\n",
      "Loss (& total) on Batch #114: 0.5431545376777649 (67.10087716579437)\n",
      "Loss (& total) on Batch #115: 0.520783543586731 (67.6216607093811)\n",
      "Loss (& total) on Batch #116: 0.5215446352958679 (68.14320534467697)\n",
      "Loss (& total) on Batch #117: 0.5234091281890869 (68.66661447286606)\n",
      "Loss (& total) on Batch #118: 0.5272232890129089 (69.19383776187897)\n",
      "Loss (& total) on Batch #119: 0.5231930613517761 (69.71703082323074)\n",
      "Loss (& total) on Batch #120: 0.5211195349693298 (70.23815035820007)\n",
      "Loss (& total) on Batch #121: 0.5299139618873596 (70.76806432008743)\n",
      "Loss (& total) on Batch #122: 0.5269296765327454 (71.29499399662018)\n",
      "Loss (& total) on Batch #123: 0.5212096571922302 (71.81620365381241)\n",
      "Loss (& total) on Batch #124: 0.5191607475280762 (72.33536440134048)\n",
      "Loss (& total) on Batch #125: 0.519803524017334 (72.85516792535782)\n",
      "Loss (& total) on Batch #126: 0.522993803024292 (73.37816172838211)\n",
      "Loss (& total) on Batch #127: 0.5176970958709717 (73.89585882425308)\n",
      "Loss (& total) on Batch #128: 0.5208871364593506 (74.41674596071243)\n",
      "Loss (& total) on Batch #129: 0.5214138627052307 (74.93815982341766)\n",
      "Loss (& total) on Batch #130: 0.5145190358161926 (75.45267885923386)\n",
      "Loss (& total) on Batch #131: 0.5158891081809998 (75.96856796741486)\n",
      "Loss (& total) on Batch #132: 0.5129651427268982 (76.48153311014175)\n",
      "Loss (& total) on Batch #133: 2.4487595558166504 (78.9302926659584)\n",
      "Loss (& total) on Batch #134: 0.5180656313896179 (79.44835829734802)\n",
      "Loss (& total) on Batch #135: 0.5223713517189026 (79.97072964906693)\n",
      "Loss (& total) on Batch #136: 0.5142043828964233 (80.48493403196335)\n",
      "Loss (& total) on Batch #137: 0.5159149169921875 (81.00084894895554)\n",
      "Loss (& total) on Batch #138: 0.5655394792556763 (81.56638842821121)\n",
      "Loss (& total) on Batch #139: 0.5188993215560913 (82.0852877497673)\n",
      "Loss (& total) on Batch #140: 0.5226033329963684 (82.60789108276367)\n",
      "Loss (& total) on Batch #141: 0.5275265574455261 (83.1354176402092)\n",
      "Loss (& total) on Batch #142: 0.5155273675918579 (83.65094500780106)\n",
      "Loss (& total) on Batch #143: 0.5183875560760498 (84.1693325638771)\n",
      "Loss (& total) on Batch #144: 0.5878193378448486 (84.75715190172195)\n",
      "Loss (& total) on Batch #145: 0.5900553464889526 (85.3472072482109)\n",
      "Loss (& total) on Batch #146: 0.5759239196777344 (85.92313116788864)\n",
      "Loss (& total) on Batch #147: 0.5150322318077087 (86.43816339969635)\n",
      "Loss (& total) on Batch #148: 0.5160037875175476 (86.9541671872139)\n",
      "Loss (& total) on Batch #149: 0.5138421058654785 (87.46800929307938)\n",
      "Loss (& total) on Batch #150: 0.5148078799247742 (87.98281717300415)\n",
      "Loss (& total) on Batch #151: 0.5288479328155518 (88.5116651058197)\n",
      "Loss (& total) on Batch #152: 0.6232609748840332 (89.13492608070374)\n",
      "Loss (& total) on Batch #153: 0.5099976062774658 (89.6449236869812)\n",
      "Loss (& total) on Batch #154: 0.5107253193855286 (90.15564900636673)\n",
      "Loss (& total) on Batch #155: 0.5394362807273865 (90.69508528709412)\n",
      "Loss (& total) on Batch #156: 0.5205596685409546 (91.21564495563507)\n",
      "Loss (& total) on Batch #157: 0.5134954452514648 (91.72914040088654)\n",
      "Loss (& total) on Batch #158: 0.6179609298706055 (92.34710133075714)\n",
      "Loss (& total) on Batch #159: 0.5075105428695679 (92.85461187362671)\n",
      "Loss (& total) on Batch #160: 0.5102147459983826 (93.36482661962509)\n",
      "Loss (& total) on Batch #161: 0.5102023482322693 (93.87502896785736)\n",
      "Loss (& total) on Batch #162: 0.5100835561752319 (94.38511252403259)\n",
      "Loss (& total) on Batch #163: 0.5081129670143127 (94.8932254910469)\n",
      "Loss (& total) on Batch #164: 2.485577344894409 (97.37880283594131)\n",
      "Loss (& total) on Batch #165: 0.5075749754905701 (97.88637781143188)\n",
      "Loss (& total) on Batch #166: 0.5631062984466553 (98.44948410987854)\n",
      "Loss (& total) on Batch #167: 0.515221357345581 (98.96470546722412)\n",
      "Loss (& total) on Batch #168: 0.5152959823608398 (99.48000144958496)\n",
      "Loss (& total) on Batch #169: 0.4980788826942444 (99.9780803322792)\n",
      "Loss (& total) on Batch #170: 0.5171383619308472 (100.49521869421005)\n",
      "Loss (& total) on Batch #171: 0.5250352621078491 (101.0202539563179)\n",
      "Loss (& total) on Batch #172: 0.5199590921401978 (101.5402130484581)\n",
      "Loss (& total) on Batch #173: 0.5267837047576904 (102.06699675321579)\n",
      "Loss (& total) on Batch #174: 0.5219792127609253 (102.58897596597672)\n",
      "Loss (& total) on Batch #175: 0.528407871723175 (103.11738383769989)\n",
      "Loss (& total) on Batch #176: 0.5291755795478821 (103.64655941724777)\n",
      "Loss (& total) on Batch #177: 0.523155152797699 (104.16971457004547)\n",
      "Loss (& total) on Batch #178: 0.5260756015777588 (104.69579017162323)\n",
      "Loss (& total) on Batch #179: 0.5357900261878967 (105.23158019781113)\n",
      "Loss (& total) on Batch #180: 0.5237643122673035 (105.75534451007843)\n",
      "Loss (& total) on Batch #181: 0.5255075693130493 (106.28085207939148)\n",
      "Loss (& total) on Batch #182: 0.5199691653251648 (106.80082124471664)\n",
      "Loss (& total) on Batch #183: 0.5180988311767578 (107.3189200758934)\n",
      "Loss (& total) on Batch #184: 0.5194167494773865 (107.83833682537079)\n",
      "Loss (& total) on Batch #185: 0.5154905319213867 (108.35382735729218)\n",
      "Loss (& total) on Batch #186: 0.5174661874771118 (108.87129354476929)\n",
      "Loss (& total) on Batch #187: 0.5142269134521484 (109.38552045822144)\n",
      "Loss (& total) on Batch #188: 0.5189093351364136 (109.90442979335785)\n",
      "Loss (& total) on Batch #189: 0.5620414614677429 (110.46647125482559)\n",
      "Loss (& total) on Batch #190: 0.5168865919113159 (110.98335784673691)\n",
      "Loss (& total) on Batch #191: 0.5149220824241638 (111.49827992916107)\n",
      "Loss (& total) on Batch #192: 0.514542281627655 (112.01282221078873)\n",
      "Loss (& total) on Batch #193: 0.5833994150161743 (112.5962216258049)\n",
      "Loss (& total) on Batch #194: 0.5103064775466919 (113.1065281033516)\n",
      "Loss (& total) on Batch #195: 0.5092729926109314 (113.61580109596252)\n",
      "Loss (& total) on Batch #196: 0.5105391144752502 (114.12634021043777)\n",
      "Loss (& total) on Batch #197: 0.5103978514671326 (114.63673806190491)\n",
      "Loss (& total) on Batch #198: 0.5095784664154053 (115.14631652832031)\n",
      "Loss (& total) on Batch #199: 0.5083432793617249 (115.65465980768204)\n",
      "Loss (& total) on Batch #200: 0.5055017471313477 (116.16016155481339)\n",
      "Loss (& total) on Batch #201: 0.506562352180481 (116.66672390699387)\n",
      "Loss (& total) on Batch #202: 0.508374810218811 (117.17509871721268)\n",
      "Loss (& total) on Batch #203: 0.5082553625106812 (117.68335407972336)\n",
      "Loss (& total) on Batch #204: 2.5688486099243164 (120.25220268964767)\n",
      "Loss (& total) on Batch #205: 0.5078431367874146 (120.76004582643509)\n",
      "Loss (& total) on Batch #206: 0.5126980543136597 (121.27274388074875)\n",
      "Loss (& total) on Batch #207: 0.5115869045257568 (121.7843307852745)\n",
      "Loss (& total) on Batch #208: 0.5138736367225647 (122.29820442199707)\n",
      "Loss (& total) on Batch #209: 0.5171701908111572 (122.81537461280823)\n",
      "Loss (& total) on Batch #210: 0.5161392688751221 (123.33151388168335)\n",
      "Loss (& total) on Batch #211: 0.5162093639373779 (123.84772324562073)\n",
      "Loss (& total) on Batch #212: 0.5251420140266418 (124.37286525964737)\n",
      "Loss (& total) on Batch #213: 0.5223347544670105 (124.89520001411438)\n",
      "Loss (& total) on Batch #214: 0.5270174741744995 (125.42221748828888)\n",
      "Loss (& total) on Batch #215: 0.7864638566970825 (126.20868134498596)\n",
      "Loss (& total) on Batch #216: 0.5226346254348755 (126.73131597042084)\n",
      "Loss (& total) on Batch #217: 0.5303202867507935 (127.26163625717163)\n"
     ]
    }
   ],
   "source": [
    "#If the kernel dies part way through training,\n",
    "# Load the most recent model and continue training\n",
    "\n",
    "# Cell 4: Initialize Model and Loss\n",
    "num_classes = 6 # full_dataset.num_classes\n",
    "\n",
    "variant = 'resnet50'   # or 'resnet18', 'resnet34', etc.\n",
    "model = ResNetTemporalClassifier(\n",
    "    num_classes=num_classes,\n",
    "    pretrained=False,      # set False if you want to train from scratch\n",
    "    variant=variant,     # options: 'resnet50', 'resnet101', 'resnet152' (but only 18,34,50 are avail pre-trained for now)\n",
    "    resize=(224, 224)        # input size for ResNet\n",
    ")\n",
    "#In the \"model\" initialization above, \n",
    "#I've set the pre-trained to false, since no internet, but will load from pre-trained models I got on my VM\n",
    "\n",
    "# Load weights\n",
    "model.load_state_dict(torch.load(\"best_model_resnet50_.pt\", map_location='cuda'))\n",
    "model = model.to('cuda')  # or 'cpu'\n",
    "\n",
    "class_weights = get_class_weights(train_dataset, num_classes)                    # FIX THIS LATER -  UNCOMMENT THIS OR DEFINE MANUALLY BUT CORRECT WEIGHTS\n",
    "print(class_weights)\n",
    "\n",
    "loss_fn = combined_loss(class_weights, alpha=config.loss_alpha)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=config.lr)\n",
    "\n",
    "# Cell 5: Train\n",
    "best_model_path = f\"best_model_{variant}.pt\"\n",
    "if USE_WANDB:\n",
    "    model          = train_model(model, train_loader, val_loader, optimizer, loss_fn, device, num_epochs=config.epochs, patience=5, USE_WANDB=USE_WANDB, best_model_path = best_model_path)\n",
    "else:\n",
    "    model, history = train_model(model, train_loader, val_loader, optimizer, loss_fn, device, num_epochs=config.epochs, patience=5, USE_WANDB=USE_WANDB, best_model_path = best_model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_loss': [0.9583881768968797, 0.8589790086111715, 0.8574826263131634, 0.8522834172171931, 0.8336483220900258, 0.8325665047572505, 0.82156113390961, 0.8176933988448112, 0.8018415827424296, 0.7963504906623594, 0.784831604890285, 0.7852150708917649, 0.7825517070389563, 0.7698381033635908, 0.7750539250912205, 0.7535960888189654, 0.751354037513656, 0.7524507615354753, 0.7423527644526574, 0.7395329321584394], 'val_loss': [0.8274070297678312, 0.792217128806644, 0.7939501139852736, 0.7924235314130783, 0.7700290646817949, 0.7498496580455039, 0.7518935592638122, 0.7520300754242473, 0.7267159074544907, 0.7344969949788518, 0.7293031530247794, 0.7422766718599532, 0.7212113978134261, 0.7469947520229552, 0.7144439145922661, 0.713933002617624, 0.7212710918651687, 0.7030177207456695, 0.7185809653666284, 0.7074187878105376], 'train_f1': [0.15546289138459493, 0.19007597932988193, 0.18941585088207896, 0.19484925846918627, 0.20334456890318764, 0.21256885120545368, 0.23521618564485366, 0.22634567532411057, 0.2715491649544336, 0.2621630557172435, 0.2749015562411371, 0.2814131341297902, 0.31988463072912465, 0.3204482853549077, 0.30468466190842747, 0.31226665712336243, 0.38499506182685805, 0.36717324022811015, 0.4400125289730048, 0.3537477269865128], 'val_f1': [0.23242367220512178, 0.2297994605371174, 0.23738921697988477, 0.30856378336932694, 0.3779347984547319, 0.3721025066563336, 0.374631244544783, 0.37627817478421866, 0.36876879357968473, 0.3843016540195331, 0.3808289054965315, 0.3034289181887267, 0.28743334418030886, 0.2965808706771399, 0.38668388379246776, 0.32001898856454736, 0.3032211027246306, 0.3364673270782751, 0.305560299966403, 0.3143395004050701]}\n"
     ]
    }
   ],
   "source": [
    "print(history)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NEXT: Train on Resnet18\n",
    "\n",
    "# Cell 4: Initialize Model and Loss\n",
    "num_classes = 6 # full_dataset.num_classes\n",
    "\n",
    "variant = 'resnet18'   # or 'resnet18', 'resnet34', etc.\n",
    "model = ResNetTemporalClassifier(\n",
    "    num_classes=num_classes,\n",
    "    pretrained=False,      # set False if you want to train from scratch\n",
    "    variant=variant,     # options: 'resnet50', 'resnet101', 'resnet152' (but only 18,34,50 are avail pre-trained for now)\n",
    "    resize=(224, 224)        # input size for ResNet\n",
    ")\n",
    "#In the \"model\" initialization above, \n",
    "#I've set the pre-trained to false, since no internet, but will load from pre-trained models I got on my VM\n",
    "\n",
    "\n",
    "# Load weights - Reload if crashes part way through\n",
    "# model.load_state_dict(torch.load(\"best_model_resnet50_.pt\", map_location='cuda'))\n",
    "# model = model.to('cuda')  # or 'cpu'\n",
    "\n",
    "# --- Load pretrained weights manually (offline) ---\n",
    "pretrained_path = f\"/lustre10/scratch/slonimer/models/{variant}.pth\"\n",
    "state_dict = torch.load(pretrained_path, map_location='cpu')\n",
    "# Filter out the fc layer weights (1000-class classifier)\n",
    "filtered_state_dict = {k: v for k, v in state_dict.items() if not k.startswith(\"fc.\")}\n",
    "# only load matching keys (to ignore classifier layer mismatches)\n",
    "missing, unexpected = model.backbone.load_state_dict(filtered_state_dict, strict=False)\n",
    "print(f\"✅ Loaded pretrained weights with {len(missing)} missing and {len(unexpected)} unexpected keys\")\n",
    "\n",
    "class_weights = get_class_weights(train_dataset, num_classes)                    # FIX THIS LATER -  UNCOMMENT THIS OR DEFINE MANUALLY BUT CORRECT WEIGHTS\n",
    "print(class_weights)\n",
    "\n",
    "loss_fn = combined_loss(class_weights, alpha=config.loss_alpha)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=config.lr)\n",
    "\n",
    "# Cell 5: Train\n",
    "\n",
    "best_model_path = f\"best_model_{variant}.pt\"\n",
    "if USE_WANDB:\n",
    "    model          = train_model(model, train_loader, val_loader, optimizer, loss_fn, device, num_epochs=config.epochs, patience=5, USE_WANDB=USE_WANDB, best_model_path = best_model_path)\n",
    "else:\n",
    "    model, history = train_model(model, train_loader, val_loader, optimizer, loss_fn, device, num_epochs=config.epochs, patience=5, USE_WANDB=USE_WANDB, best_model_path = best_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(history)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3219469/1187203441.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"best_model_20250508.pt\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       817\n",
      "           1       0.98      0.98      0.98        47\n",
      "\n",
      "    accuracy                           1.00       864\n",
      "   macro avg       0.99      0.99      0.99       864\n",
      "weighted avg       1.00      1.00      1.00       864\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Load Best Model and Evaluate\n",
    "model.load_state_dict(torch.load(\"best_model_20250508.pt\"))\n",
    "#model.load_state_dict(torch.load(\"best_model.pt\"))\n",
    "model.eval()\n",
    "\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "#for x, y in val_loader:\n",
    "for x, y in test_loader:\n",
    "    x = x.to(device)\n",
    "    with torch.no_grad():\n",
    "        out = model(x)\n",
    "        #Need to reshape the outputs, and y to match dimensions:\n",
    "        out = out.reshape(-1, out.shape[-1])  # (B*T, num_classes)\n",
    "        #The prediction is the class with largest score per sample\n",
    "        preds = torch.argmax(out, dim=1)\n",
    "\n",
    "    #Need to reshape the outputs, and y to match dimensions:\n",
    "    y = y.view(-1)                        # (B*T, )\n",
    "\n",
    "    #Append the results\n",
    "    all_preds.append(preds.cpu())\n",
    "    all_labels.append(y)\n",
    "\n",
    "y_pred = torch.cat(all_preds).numpy()\n",
    "y_true = torch.cat(all_labels).numpy()\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_true, y_pred))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "#Best model 2025-05-08\n",
    "\n",
    "val_loader:\n",
    "    \n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       1.00      1.00      1.00    684575\n",
    "           1       0.78      0.98      0.87      1913\n",
    "           2       0.00      0.00      0.00       383\n",
    "           3       0.44      0.89      0.59         9\n",
    "\n",
    "    accuracy                           1.00    686880\n",
    "   macro avg       0.56      0.72      0.62    686880\n",
    "weighted avg       1.00      1.00      1.00    686880\n",
    "\n",
    "\n",
    "\n",
    "test_loader:\n",
    "\n",
    "           0       1.00      1.00      1.00    337877\n",
    "           1       0.73      0.95      0.83      1257\n",
    "           2       0.00      0.00      0.00       418\n",
    "           3       0.00      0.00      0.00         0\n",
    "\n",
    "    accuracy                           1.00    339552\n",
    "   macro avg       0.43      0.49      0.46    339552\n",
    "weighted avg       1.00      1.00      1.00    339552\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1775809/1511681119.py:31: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path + \"best_model_20250508.pt\" ))\n"
     ]
    }
   ],
   "source": [
    "#Cell 7: Make test plots.  \n",
    "#I want to push a file through the algorithm, and see how it performs\n",
    "\n",
    "#IMPORT EVERYTHING:\n",
    "import os\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "from src.dataset_loader import ADCPDataset  # your custom dataset\n",
    "from src.model import TemporalCNN # CNNClassifier  # your model\n",
    "from src.utils import seed_everything, get_class_weights, combined_loss, train_model\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "'''\n",
    "#DEBUG:\n",
    "#I was having bugs in this, where it was saying it failed creating a primitive. This was found to solve the issue (but shouldn't be used when proper training/running)\n",
    "import os\n",
    "os.environ[\"TORCH_DISABLE_MKL\"] = \"1\"  # optional: disables MKL\n",
    "os.environ[\"ONEDNN_VERBOSE\"] = \"0\"\n",
    "os.environ[\"DNNL_VERBOSE\"] = \"0\"\n",
    "torch.backends.mkldnn.enabled = False\n",
    "'''\n",
    "\n",
    "\n",
    "# INITIALIZE THE MODEL\n",
    "model_path = \"/lustre10/scratch/slonimer/ML_ADCP/\"\n",
    "#model_path = \"/scratch/slonimer/ML_ADCP/\"\n",
    "num_classes = 6 \n",
    "#model = TemporalCNN(input_channels=3, num_classes=num_classes)\n",
    "model = ResNetTemporalClassifier(\n",
    "    num_classes=num_classes,\n",
    "    pretrained=True,      # set False if you want to train from scratch\n",
    "    variant='resnet50',     # options: 'resnet50', 'resnet101', 'resnet152'\n",
    "    resize=(224, 224)        # input size for ResNet\n",
    ")\n",
    "\n",
    "# model.load_state_dict(torch.load(model_path + \"best_model_20250505.pt\" ))\n",
    "model.load_state_dict(torch.load(model_path + \"best_model_20250508.pt\" ))\n",
    "model.eval()\n",
    "\n",
    "\n",
    "#Specify the data to use:\n",
    "file_path = \"/lustre10/scratch/slonimer/ML_ADCP/BACAX_24hr_h5/\"\n",
    "#file_path = \"/scratch/slonimer/ML_ADCP/BACAX_24hr_h5/\"\n",
    "h5_filename = '20240406T000000_20240406T235959.h5'\n",
    "#h5_filename = '20230701T000230_20230702T000229.h5'\n",
    "h5_test_file = []\n",
    "# h5_test_file.append(file_path + '20230701T000230_20230702T000229.h5') \n",
    "h5_test_file.append(file_path + h5_filename) \n",
    "\n",
    "\n",
    "#CLASSIFY THE TEST FILE\n",
    "def classify_test_data(model, h5_test_file):\n",
    "    test_file_dataset = ADCPDataset(h5_test_file)\n",
    "    test_loader = DataLoader(test_file_dataset, batch_size=3, shuffle=False, num_workers=4)\n",
    "\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "\n",
    "    for x, y in test_loader:\n",
    "        x = x.to(device)\n",
    "        model = model.to(device)  # ← Add this line\n",
    "        with torch.no_grad():\n",
    "            out = model(x)\n",
    "            #Need to reshape the outputs, and y to match dimensions:\n",
    "            out = out.reshape(-1, out.shape[-1])  # (B*T, num_classes)\n",
    "            #The prediction is the class with largest score per sample\n",
    "            preds = torch.argmax(out, dim=1)\n",
    "\n",
    "        #Need to reshape the outputs, and y to match dimensions:\n",
    "        y = y.view(-1)                        # (B*T, )\n",
    "\n",
    "        #Append the results\n",
    "        all_preds.append(preds.cpu())\n",
    "        all_labels.append(y)\n",
    "        \n",
    "    return x, y, preds\n",
    "\n",
    "\n",
    "#Run the classification\n",
    "x, y, preds = classify_test_data(model, h5_test_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       817\n",
      "           1       0.98      0.98      0.98        47\n",
      "\n",
      "    accuracy                           1.00       864\n",
      "   macro avg       0.99      0.99      0.99       864\n",
      "weighted avg       1.00      1.00      1.00       864\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y.cpu(), preds.cpu()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def get_segments(annotations, ann):\n",
    "    if ann==1: #For annotations (may be multiclass)\n",
    "        mask = np.diff(annotations) != 0 # Create a mask of where changes in annotations are non-zero\n",
    "        #diffs = mask.astype(int)\n",
    "    elif ann==0: #For predictions\n",
    "        mask = annotations != 0 # Create a mask of where annotations are non-zero\n",
    "        \n",
    "    diffs = np.diff(mask.astype(int)) # Find the changes in the mask\n",
    "    start_indices = np.where(diffs == 1)[0] + 1 # Start indices: where diff == 1 (0 → 1)\n",
    "    end_indices = np.where(diffs == -1)[0] + 1 # End indices: where diff == -1 (1 → 0)\n",
    "    # Handle edge cases: \n",
    "    if mask[0]: #if it starts with a non-zero \n",
    "        start_indices = np.r_[0, start_indices]\n",
    "\n",
    "    if mask[-1]: # if it ends with a non-zero\n",
    "        end_indices = np.r_[end_indices, len(annotations)]\n",
    "\n",
    "    anomaly_segments = list(zip(start_indices, end_indices)) # Zip together\n",
    "    return anomaly_segments\n",
    "\n",
    "\n",
    "def plot_results(x, annotations, predictions, filename) :\n",
    "    x = x.cpu()\n",
    "    annotations = annotations.cpu()\n",
    "    predictions = predictions.cpu()\n",
    "\n",
    "    n_beams = x.shape[0]#[2]\n",
    "    n_channels = x.shape[1]#[2]\n",
    "\n",
    "    for beam in range(n_beams):\n",
    "        fig, axs = plt.subplots(n_channels, 1, sharex=True, figsize=(12, 2.5*n_channels))\n",
    "        if n_channels == 1:\n",
    "            axs = [axs]\n",
    "\n",
    "        #Get the annotations for this beam\n",
    "        anomaly_segments = get_segments(annotations[beam].cpu().numpy(),ann = 1)\n",
    "        pred_segments = get_segments(predictions[beam].cpu().numpy(), ann = 0)\n",
    "\n",
    "        #print(anomaly_segments)\n",
    "        #print(pred_segments)\n",
    "\n",
    "        #Determine if any annotations present in this beam:\n",
    "        cls_str = '' #Initialize as nothing\n",
    "        ann = annotations[beam].cpu().numpy()\n",
    "        if np.any(ann>0):\n",
    "            cls = np.median(ann[ann>0])\n",
    "            cls_str = ', class: {}'.format(int(cls))\n",
    "\n",
    "\n",
    "        #Plot Velocity, backscatter, and correlation, for each beam\n",
    "        for ch in range(n_channels):\n",
    "            #Plot the Complex Data\n",
    "            im = axs[ch].imshow(\n",
    "                x[beam,ch,:,:].T, aspect='auto', origin='lower',\n",
    "                #extent=[extent[0], extent[1], extent[2], extent[3]],\n",
    "                #extent=[t_hours[0], t_hours[-1], 0, arrp.shape[1]-1],\n",
    "                interpolation='nearest',\n",
    "                cmap='jet',\n",
    "            )\n",
    "\n",
    "            #Set the figure title\n",
    "            if beam == 0 and ch == 0:\n",
    "                fig.suptitle('File: {}'.format(filename))\n",
    "            \n",
    "            #Set the subplot titles\n",
    "            if ch == 0:\n",
    "                axs[ch].set_title('Beam #{} {}'.format(beam+1, cls_str))   \n",
    "           \n",
    "            #Add labels and titles\n",
    "            #axs[ch].set_ylabel(\"Range bin\" if range_dim is not None else '')\n",
    "            #axs[ch].set_title(f\"{var} - Channel {ch+1}\")\n",
    "\n",
    "            #Add dashed vertical lines for annotations\n",
    "            for start, end in anomaly_segments:\n",
    "                if 0 <= start < x.shape[2]:\n",
    "                    axs[ch].axvline(x=start, color='red', linestyle='dashed', alpha=0.7)\n",
    "                if 0 <= end < x.shape[2]:\n",
    "                    axs[ch].axvline(x=end, color='red', linestyle='dashed', alpha=0.7)\n",
    "\n",
    "            #Add shading for predictions\n",
    "            for start, end in pred_segments:\n",
    "                if 0 <= start < x.shape[2] and 0 <= end <= x.shape[2]:\n",
    "                    axs[ch].axvspan(start, end, color='black', alpha=0.3)\n",
    "\n",
    "            #Add a colorbar\n",
    "            fig.colorbar(im, ax=axs[ch], label='color')\n",
    "\n",
    "        # -- Date formatting for X --\n",
    "        axs[-1].xaxis_date()  # tells matplotlib to interpret x as dates\n",
    "        axs[-1].xaxis.set_major_formatter(mdates.DateFormatter('%H:%M'))\n",
    "        fig.autofmt_xdate()  # Makes dates pretty (auto-rotates, etc.)\n",
    "\n",
    "        #axs[-1].set_xlabel(time_dt[0].astype('datetime64[D]').astype(str))   # 'yyyy-mm-dd' date for xlabel\n",
    "        #axs[-1].set_xlabel(\"Time (hours since start)\")\n",
    "        #fig.suptitle(f\"{var} (shape={arr.shape})\")\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        #if outdir:\n",
    "        #    if not os.path.exists(outdir):\n",
    "        #        os.makedirs(outdir)\n",
    "        #    plt.savefig(f\"{outdir}/{var}.png\", dpi=120)\n",
    "        #if show:\n",
    "        #    plt.show()\n",
    "        plt.show()\n",
    "        #plt.close()\n",
    "\n",
    "        \n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get indices of start/end segments of anomalies\n",
    "annotations = y.view(3,288)\n",
    "predictions = preds.view(3,288)\n",
    "\n",
    "plot_results(x, annotations, predictions, h5_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#For ALL anomalous files, plot the annotations and the prediction\n",
    "\n",
    "# Load anomaly filenames from a text file\n",
    "with open(\"annotated_files.txt\", \"r\") as f:\n",
    "    anomaly_files = set(line.strip() for line in f if line.strip())\n",
    "    \n",
    "#Define anomaly paths before truncating h5_paths\n",
    "file_path = \"/scratch/slonimer/ML_ADCP/BACAX_24hr_h5/\"\n",
    "anomaly_paths = []\n",
    "for filename in anomaly_files:\n",
    "    anomaly_paths.append(file_path + filename)\n",
    "    \n",
    "#    anomaly_paths = [p for p in h5_paths if os.path.basename(p) in anomaly_files]\n",
    "\n",
    "#Run the classification\n",
    "for anomaly_path in anomaly_paths:\n",
    "    print(anomaly_path)\n",
    "    #Predict the class\n",
    "    x, y, preds = classify_test_data(model, [anomaly_path])\n",
    "    \n",
    "    #Get indices of start/end segments of anomalies\n",
    "    annotations = y.view(3,288)\n",
    "    predictions = preds.view(3,288)\n",
    "\n",
    "\n",
    "    #Plot the results\n",
    "    plot_results(x, annotations, predictions, os.path.basename(anomaly_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Look for False positives!\n",
    "\n",
    "#Push all files through. If any are classified as drop-outs with more than 6 in a row (half an hour), make a plot\n",
    "\n",
    "#Get a list of files from the directory:\n",
    "data_folder = \"/scratch/slonimer/ML_ADCP/BACAX_24hr_h5/\"\n",
    "#data_folder= r\"F:\\Documents\\Projects\\ML\\ADCP_ML\\h5_24h_files\\\\\"\n",
    "file_list = os.listdir(data_folder)\n",
    "h5_files = {k for k in file_list if os.path.splitext(k)[1] == \".h5\"}\n",
    "#Files are inherently NOT in order in python! So if you want them in order, need to do this:\n",
    "h5_files = sorted(h5_files)  # Sorts alphabetically\n",
    "\n",
    "h5_paths = []\n",
    "for filename in h5_files:\n",
    "    h5_paths.append(data_folder + filename) \n",
    "\n",
    "'''\n",
    "#For ALL anomalous files, plot the annotations and the prediction\n",
    "\n",
    "# Load anomaly filenames from a text file\n",
    "with open(\"annotated_files.txt\", \"r\") as f:\n",
    "    anomaly_files = set(line.strip() for line in f if line.strip())\n",
    "    \n",
    "#Define anomaly paths before truncating h5_paths\n",
    "file_path = \"/scratch/slonimer/ML_ADCP/BACAX_24hr_h5/\"\n",
    "anomaly_paths = []\n",
    "for filename in anomaly_files:\n",
    "    anomaly_paths.append(file_path + filename)\n",
    "    \n",
    "#    anomaly_paths = [p for p in h5_paths if os.path.basename(p) in anomaly_files]\n",
    "'''\n",
    "\n",
    "#Run the classification\n",
    "for file_path in h5_paths:\n",
    "    print(file_path)\n",
    "    #Predict the class\n",
    "    x, y, preds = classify_test_data(model, [file_path])\n",
    "    \n",
    "    #Get indices of start/end segments of anomalies\n",
    "    annotations = y.view(3,288)\n",
    "    predictions = preds.view(3,288)\n",
    "\n",
    "    \n",
    "    #Determine if any annotations present in any beam:\n",
    "    do_plot = 0\n",
    "    for beam in range(3):\n",
    "        ann_beam = annotations[beam].cpu().numpy()\n",
    "        pred_beam = predictions[beam].cpu().numpy()\n",
    "        \n",
    "        #If more than one hour predicted in a day in any beam:\n",
    "        n_samples = 24 # 1 hour\n",
    "        #n_samples = 12 # 1 hour\n",
    "        if np.all(ann_beam==0) and np.sum(pred_beam>0)>n_samples:\n",
    "            do_plot = 1\n",
    "    \n",
    "    if do_plot == 1:\n",
    "        #Plot the results\n",
    "        plot_results(x, annotations, predictions, os.path.basename(file_path))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "adcp_anomaly_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
